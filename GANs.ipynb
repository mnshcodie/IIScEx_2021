{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnshcodie/IIScEx_2021/blob/main/GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pGc7xQ2dSRL"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* Understand GAN\n",
        "* Generate fake images of MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NuRcSZydkL_"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KqbKyGSdnuq"
      },
      "source": [
        "###Description\n",
        "\n",
        "We use the MNIST dataset for this experiment. Below are the details:\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, \n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately). \n",
        "2. Each image is Size Normalized and Centered \n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
        "\n",
        "### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJu4cGUCdwTr"
      },
      "source": [
        "## Domain Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMtA8wDddypU"
      },
      "source": [
        "\n",
        "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
        "\n",
        "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4F3RvUAd4Hw"
      },
      "source": [
        "## AI / ML Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1qJVLV4f2tw"
      },
      "source": [
        "### Generative Adversary Networks (GAN)\n",
        "\n",
        "\n",
        "GANs are generative models devised by Goodfellow et al. in 2014. GAN is about creating, like drawing a portrait or composing a symphony. This is hard compared to other deep learning fields. For instance, it is much easier to identify a Monet painting than painting one.\n",
        "\n",
        "\n",
        "The main focus of GAN is to generate data from scratch, mostly images but other domains including music have been done.\n",
        "\n",
        "GAN composes of two deep networks :\n",
        "\n",
        "* Generator\n",
        "* Discriminator\n",
        "\n",
        "\n",
        "#### Generator \n",
        "\n",
        "The generator tries to produce data that come from some probability distribution. For example, that would be you trying to reproduce the party’s tickets.\n",
        "\n",
        "#### Discriminator\n",
        "\n",
        "The discriminator acts like a judge. It gets to decide if the input comes from the generator or from the true training set. For example, that would be the party’s security comparing your fake ticket with the true ticket to find flaws in your design.\n",
        "\n",
        "In summary, we can say that :\n",
        "\n",
        "* The generator trying to maximize the probability of making the discriminator mistake its inputs as real.\n",
        "\n",
        "* And the discriminator guiding the generator to produce more realistic images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfpAgIPxr1Em"
      },
      "source": [
        "![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/gan.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBD0M7uyinT3"
      },
      "source": [
        "### Importing required  Packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0WQEuRFNF8A"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "# Pytorch Packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJJ-dZUNQ7G"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lk_QTO1NLqg"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data/', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTlJWI8NVX7"
      },
      "source": [
        "### Defining the Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6WC2JuiNShJ"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.model(x.view(x.size(0), 784))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyQx9eGNWPQ"
      },
      "source": [
        "### Defining the Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtiXrEclNW1p"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 100)\n",
        "        out = self.model(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPeKiV1uTnZ"
      },
      "source": [
        "### Initializing the CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eji1M8AFN5wW"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOCjnR7pNZwG"
      },
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "generator = Generator().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWV3XqY5OMVQ"
      },
      "source": [
        "### Defining the Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kBbD1H9OJVx"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "lr = 0.0002\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8kVlovDORTG"
      },
      "source": [
        "### Training Discriminator Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cerV5nOKOOoz"
      },
      "source": [
        "def train_discriminator(discriminator, images, real_labels, fake_images, fake_labels):\n",
        "    # Train on real images\n",
        "    # Reset gradients\n",
        "    discriminator.zero_grad()\n",
        "    prediction_real = discriminator(images)\n",
        "    \n",
        "    # Comparing original images \n",
        "    real_loss = criterion(prediction_real, real_labels)\n",
        "    \n",
        "    # Train on fake images\n",
        "    prediction_fake = discriminator(fake_images) \n",
        "    \n",
        "    # Comparing fake images\n",
        "    # Calculate error and backpropagate\n",
        "    fake_loss = criterion(prediction_fake, fake_labels)\n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "\n",
        "    # Update weights with gradients\n",
        "    d_optimizer.step()\n",
        "    return d_loss, prediction_real, prediction_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucVw5ikMxFZx"
      },
      "source": [
        "### Training Generator Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lpHk9QOZ52"
      },
      "source": [
        "def train_generator(generator, fake_images, real_labels):\n",
        "    # Train Generator\n",
        "    # Reset gradients\n",
        "    generator.zero_grad()\n",
        "\n",
        "    # Sample noise and generate fake data\n",
        "    discriminator_outputs = discriminator(fake_images)\n",
        "\n",
        "    # Calculate error and backpropagate\n",
        "    g_loss = criterion(discriminator_outputs, real_labels)\n",
        "    g_loss.backward()\n",
        "\n",
        "    # Update weights with gradients\n",
        "    g_optimizer.step()\n",
        "    \n",
        "    # Return error\n",
        "    return g_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoSFz9yu_hwx"
      },
      "source": [
        "### Generate Samples for Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf9-TwixOihO"
      },
      "source": [
        "# Draw samples from the input distribution to inspect the generation on training \n",
        "num_test_samples = 16\n",
        "test_noise = torch.randn(num_test_samples, 100).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3929IPDf_MD"
      },
      "source": [
        "### Lets start training the GAN, in below output the image gets updated for every iteration\n",
        "\n",
        "**Note:** The below training process for 150 epochs will take around 1hr 15 mins to complete the execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqdz84moOnxw"
      },
      "source": [
        "# Create figure for plotting\n",
        "size_figure_grid = int(math.sqrt(num_test_samples))\n",
        "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n",
        "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "    ax[i,j].get_xaxis().set_visible(False)\n",
        "    ax[i,j].get_yaxis().set_visible(False)\n",
        "\n",
        "# Set number of epochs and initialize figure counter\n",
        "num_epochs = 150    # Increase the no of epochs\n",
        "num_batches = len(train_loader)\n",
        "num_fig = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for n, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        real_labels = torch.ones(images.size(0), 1).to(device)\n",
        "        \n",
        "        # Sample from generator\n",
        "        # Generate fake images by passing the random noise vector to the generator\n",
        "        noise = torch.randn(images.size(0), 100).to(device)\n",
        "        fake_images = generator(noise)\n",
        "        fake_labels = torch.zeros(images.size(0), 1).to(device)\n",
        "        \n",
        "        # Train the discriminator\n",
        "        d_loss, d_pred_real, d_pred_fake = train_discriminator(discriminator, images, real_labels, fake_images, fake_labels)\n",
        "        \n",
        "        # Sample again from the generator \n",
        "        noise = torch.randn(images.size(0), 100).to(device)\n",
        "        fake_images = generator(noise)\n",
        "\n",
        "        # Train the generator\n",
        "        g_loss = train_generator(generator, fake_images, real_labels)\n",
        "        \n",
        "        if (n+1) % 100 == 0:\n",
        "            test_images = generator(test_noise)\n",
        "            \n",
        "            for k in range(num_test_samples):\n",
        "                i = k//4\n",
        "                j = k%4\n",
        "                ax[i,j].cla()\n",
        "                ax[i,j].imshow(test_images[k,:].data.cpu().numpy().reshape(28, 28), cmap='Greys')\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf())\n",
        "            \n",
        "            plt.savefig('results/mnist-gan-%03d.png')\n",
        "            num_fig += 1\n",
        "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
        "                  'D(x): %.2f, D(G(z)): %.2f' \n",
        "                  %(epoch + 1, num_epochs, n+1, num_batches, d_loss.item(), g_loss.item(),\n",
        "                    d_pred_real.data.mean(), d_pred_fake.data.mean()))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}