{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "M3_AST_15_Logistic_Regression_&_MLP_&_BP_B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnshcodie/IIScEx_2021/blob/main/M3_AST_15_Logistic_Regression_%26_MLP_%26_BP_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 15 : Logistic Regression and Multi-Layer Perceptron using Keras and Back Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "   \n",
        "  At the end of the experiment, you will be able to :\n",
        "    \n",
        "  * implement Logistic Regression using python code and sklearn library\n",
        "  * understand the concept of Multi Layer Perceptron (MLP)\n",
        "  * build an image classifier using the Keras Sequential API\n",
        "  * understand the backpropagation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAsQn9McNyVD"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The dataset named \"Heart Disease Dataset\" comes from a study conducted in 1988 and originates from the UCI Machine Learning Repository. The task is to get the best predictor and guess if a patient has a heart disease.\n",
        "\n",
        "The dataset consists of 303 individuals data. There are 14 columns in the dataset, which are described below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "503VK75SUE11"
      },
      "source": [
        "**1. Age:** The person‚Äôs age in years\n",
        "\n",
        "**2. Sex:** The person‚Äôs Gender (1 = male, 0 = female)\n",
        "\n",
        "**3. cp - chest pain type:** The type of chest pain experienced by the individual person\n",
        "\n",
        "\n",
        "\n",
        "*  0: typical angina \n",
        "\n",
        "*  1: atypical angina\n",
        "\n",
        "*  2: non-anginal pain\n",
        "\n",
        "*  3: asymptotic\n",
        "\n",
        "\n",
        "**4. trestbps - Resting Blood Pressure:** The person‚Äôs resting blood pressure (mm Hg on admission to the hospital)\n",
        "\n",
        "**5. chol - Serum Cholestrol:** The person‚Äôs cholesterol measurement in mg/dl\n",
        "\n",
        "**6. fbs - Fasting Blood Sugar:** The person‚Äôs fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
        "\n",
        "**7. restecg - Resting ECG:** resting electrocardiographic results\n",
        "\n",
        "*   0: normal\n",
        "*   1: having ST-T wave abnormality\n",
        "*   2: left ventricular hyperthrophy\n",
        "\n",
        "\n",
        "**8. thalach - Max heart rate achieved:** The person‚Äôs maximum heart rate achieved\n",
        "\n",
        "**9. exang - Exercise induced angina:** Exercise induced angina (1 = yes; 0 = no)\n",
        "\n",
        "**10. oldpeak - ST depression induced by exercise relative to rest:** ST depression induced by exercise relative to rest (‚ÄòST‚Äô relates to positions on the ECG plot.)\n",
        "\n",
        "**11. slope - Peak exercise ST segment:** The slope of the peak exercise ST segment \n",
        "\n",
        "\n",
        "*  0: downsloping\n",
        "*  1: flat\n",
        "*  2: upsloping\n",
        "\n",
        "\n",
        "**12. ca - Number of major vessels (0‚Äì3) colored by flourosopy:** The number of major vessels (0‚Äì3)\n",
        "\n",
        "**13. thal:** A blood disorder called thalassemia \n",
        "\n",
        "\n",
        "*   0: NULL (dropped from the dataset)\n",
        "*   1: fixed defect (no blood flow in some part of the heart)\n",
        "*   2: normal blood flow\n",
        "*   3: reversible defect (a blood flow is observed but it is not normal)\n",
        "\n",
        "**14. target:** Heart disease (1 = no, 0= yes)\n",
        "\n",
        "Data source to this experiment : http://archive.ics.uci.edu/ml/datasets/statlog+(heart)\n",
        "\n",
        "\n",
        "\n",
        "**Problem description:**\n",
        "\n",
        "The goal is to predict the binary class Heart Disease (target), which represents whether or not a patient has heart disease:\n",
        "\n",
        "0 represents no heart disease present\n",
        "\n",
        "1 represents heart disease present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exG368oL8jv2",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_15_Logistic_Regression_&_MLP_&_BP_B\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Heart_Disease.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Housing_data.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1wljNTNvmIr"
      },
      "source": [
        "### Importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TaICL_cI8yX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LH0pQnKI8yb"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5pC_cikm-Ai"
      },
      "source": [
        "df = pd.read_csv('Heart_Disease.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-lmLLgFz2m5"
      },
      "source": [
        "# Check for the shape of the dataset\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQD56JkD15MK"
      },
      "source": [
        "Check for the duplicate rows in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XSvL0SM1YD2"
      },
      "source": [
        "df[df.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KCcxaNuzs0c"
      },
      "source": [
        "# Drop the duplicated row from the dataset\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp2Jwb5nz6hc"
      },
      "source": [
        "# Check for the shape after removing the duplicated row\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_p3v_-9zfH6"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-H8ueX2hR3"
      },
      "source": [
        "The info() function is used to print a concise summary of a Data Frame. This method prints information about a Data Frame including the index dtype and column dtypes, non-null values, and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Siwajp3x2I0_"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apDG23Jt2qEW"
      },
      "source": [
        "The describe() function computes a summary of statistics pertaining to the Data Frame columns. This function gives the mean, std values. And, function excludes the character columns and given a summary about numeric columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBy-GjVoAAhS"
      },
      "source": [
        "# Check for missing values\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxcKaiCh2M_J"
      },
      "source": [
        "There are no missing values in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHaDLvw-Utq"
      },
      "source": [
        "# Check for the target values\n",
        "df['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNATML_i5TMr"
      },
      "source": [
        "tar = df['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHPVjjP23Kqy"
      },
      "source": [
        "Countplot shows the counts of observations in each categorical bin using bars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lw6hjpx-8EM"
      },
      "source": [
        "# Visualization of the target column\n",
        "sns.countplot(df['target']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJgtnH9F_KPS"
      },
      "source": [
        "Here ‚Äú1‚Äù is the number of people suffering from heart disease and ‚Äú0‚Äù is the number of people who are not suffering from heart disease. Hence the number of people suffering from heart disease is ‚Äú165‚Äù and the number of people not suffering from heart disease is ‚Äú138‚Äù.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd00lXpx-RSN"
      },
      "source": [
        "### Exploratory Data Analysis \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ODP4J8fcX5S"
      },
      "source": [
        "# Plot the histogram to see the distribution of each column in the data\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px3p_pGq_MPi"
      },
      "source": [
        "# Percentage of patients with and without hear disease\n",
        "count_No_Disease = len(df[df.target == 0])\n",
        "count_Have_Disease = len(df[df.target == 1])\n",
        "print(\"Percentage of Patients without Heart Disease: {:.2f}%\".format((count_No_Disease / (len(df.target))*100)))\n",
        "print(\"Percentage of Patients with Heart Disease: {:.2f}%\".format((count_Have_Disease / (len(df.target))*100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBR-rc823ctR"
      },
      "source": [
        "Here we find the percentage of people that are suffering and the people who are not suffering from heart disease and they are 45.54% and 54.45% respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeAWeRxjKfv"
      },
      "source": [
        "# Preparing data for the visualization\n",
        "\n",
        "df2 = df.copy()\n",
        "\n",
        "def converter(sex):\n",
        "    if sex == 0:\n",
        "        return 'female'\n",
        "    else:\n",
        "        return 'male'\n",
        "\n",
        "df2['sex'] = df2['sex'].apply(converter)\n",
        "\n",
        "def converter2(prob):\n",
        "    if prob == 0:\n",
        "        return 'Heart Disease'\n",
        "    else:\n",
        "        return 'No Heart Disease'\n",
        "\n",
        "df2['target'] = df2['target'].apply(converter2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd8HcEle3mC6"
      },
      "source": [
        "# Check for the unique values for Gender\n",
        "df2['sex'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csj1BR67h2Mr"
      },
      "source": [
        "#### Countplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvUbfRGD_nHi"
      },
      "source": [
        "# YOUR CODE HERE: Countplot Gender vs Target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJnqE-zn30q7"
      },
      "source": [
        "Here, by this count plot, we can see that number of females is less as compared to the number of \n",
        "males and we can easily see that the proportion of men suffering from heart disease is more than that of females."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUDkLO_S_udh"
      },
      "source": [
        "count_Female = len(df[df.sex == 0])\n",
        "count_Male = len(df[df.sex == 1])\n",
        "print(\"Percentage of Female Patients: {:.2f}%\".format((count_Female / (len(df.sex))*100)))\n",
        "print(\"Percentage of Male Patients: {:.2f}%\".format((count_Male / (len(df.sex))*100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55t1vKQC8Nu8"
      },
      "source": [
        "# Countplot to check for the count of types of chest pain \n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyCkhtux8ZtM"
      },
      "source": [
        "Here, from the above plot, we can see that most of the patients have typical anginal chest pain whereas very few patients suffer from an asymptotic type of chest pain. Here, we can easily see that people having typical anginal pain are much less likely to have heart problems as compared to the rest of the three."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZM7_0V3VtY8"
      },
      "source": [
        "sns.countplot(data= df2, x='sex',hue='thal')\n",
        "plt.title('Gender v/s Thalassemia\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn2a2kVrVVJc"
      },
      "source": [
        "Thalassemia (thal-uh-SEE-me-uh) is an inherited blood disorder that causes your body to have less hemoglobin than normal. Hemoglobin enables red blood cells to carry oxygen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9T1H5dkk5o"
      },
      "source": [
        "The Beta thalassemia cardiomyopathy is mainly characterized by two distinct pheno types , dilated type, with left ventricular dilatation and impaired contractility and a restrictive pheno type, with restrictive left ventricular feeling , pulmonary hyper tension and right heart failure. Heart problems, congestive heart failures and abnormal heart rhythms can be associated with severe thalassemia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz2Kt3auknZu"
      },
      "source": [
        "# YOUR CODE HERE: Countplot Slope vs Target;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXymGGBRBLDU"
      },
      "source": [
        "Here we can clearly see people having slope ‚Äú1‚Äù and slope ‚Äú2‚Äù is much more than slope ‚Äú0‚Äù. From the above plot, we can easily see that people having slope ‚Äú2‚Äù have much more heart problems as compared to slope ‚Äú0‚Äù and slope ‚Äú1‚Äù."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hiE9jpxPX_Z"
      },
      "source": [
        "#### Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqf5T0hvFbGS"
      },
      "source": [
        "Let us look at the people‚Äôs age who are suffering from the disease or not.\n",
        "Here, target = 1 implies that the person is suffering from heart disease and target = 0 implies the person is not suffering.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzfpfifoAMX-"
      },
      "source": [
        "pd.crosstab(df.age, tar).plot(kind=\"bar\",figsize=(20,6))\n",
        "plt.title('Heart Disease Frequency for Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTzLGbUeFkRi"
      },
      "source": [
        "We see that most people who are suffering are of the age of 58, followed by 57.\n",
        "Majorly, people belonging to the age group 50+ are suffering from the disease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_KjC5ycWLK_"
      },
      "source": [
        "### Distplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTWUy1XQWV0R"
      },
      "source": [
        "plt.figure(figsize=(16,7))\n",
        "sns.distplot(df[df['target']==0]['age'], kde=False, bins=50)\n",
        "plt.title('Age of Heart Diseased Patients');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXJxzxWrWc-c"
      },
      "source": [
        "From the above plot we can observe that Heart Disease is very common in the seniors which is composed of age group 60 and above and common among adults which belong to the age group of 41 to 60. But it‚Äôs rare among the age group of 19 to 40 and very rare among the age group of 0 to 18."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTx-rmk1l1jK"
      },
      "source": [
        "# YOUR CODE HERE: Distribution plot of Chol vs target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdqaPsFxl63-"
      },
      "source": [
        "Total cholesterol\n",
        "\n",
        "* LDL ‚Äî bad cholesterol\n",
        "* HDL ‚Äî good cholesterol\n",
        "\n",
        "In adults, the total cholesterol levels are considered desirable less than 200 milligram per decilitre ( mg / dL). Borderlines are considered to be high between 200 to 239 mg / dL and 240 mg / dL and above. LDL should contain less than 100 mg / dL of cholesterol. 100 mg / dl rates for individuals without any health issue are appropriate but may be more relevant for those with cardiac problems or risk factors for heart disease. The levels are borderline moderate between 130 and 159 mg / dL and moderate between 160 and 189 mg / dL. The reading is very high at or above 190 mg / dL. Levels of HDL are to be maintained higher. The risk factor for cardiovascular diseases is called a reading less than 40 mg / dL. Borderline low is considered to be between 41 mg / dL and 59 mg / dL. The HDL level can be measured with a maximum of 60 mg / dL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ve7jmdkPTE_"
      },
      "source": [
        "#### Correlation Plot\n",
        "\n",
        "From the below matrix, we observe that the highly correlated features has the correlation value close to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUzR7OTtG-x5"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jL5b5_kX5nf"
      },
      "source": [
        "### Storing the features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clR9VMVlXtPh"
      },
      "source": [
        "# YOUR CODE HERE: Storing features\n",
        "# YOUR CODE HERE: Storing label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd22mtSp9gTc"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D5l0XFXYQLh"
      },
      "source": [
        "### Normalization of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzhpdBRaxPLv"
      },
      "source": [
        "scaler = MinMaxScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV86WxDp9dJP"
      },
      "source": [
        "X[[\"age\"]] = scaler.fit_transform(X[[\"age\"]])\n",
        "X[[\"trestbps\"]] = scaler.fit_transform(X[[\"trestbps\"]])\n",
        "X[[\"chol\"]] = scaler.fit_transform(X[[\"chol\"]])\n",
        "X[[\"thalach\"]] = scaler.fit_transform(X[[\"thalach\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p49qG0mlYFsz"
      },
      "source": [
        "### Splitting the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AGWCsnYHzf"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqWcQyhbCxmX"
      },
      "source": [
        "### Logistic Regression from scratch using gradient method \n",
        "\n",
        "#### What is Logistic Regression?\n",
        "\n",
        "* Logistic Regression is a Supervised statistical technique to find the probability of dependent variable(Classes present in the variable).\n",
        "\n",
        "* Logistic regression uses function called the logit function,that helps derive a relationship between the dependent variable and independent variables by predicting the probabilities or chances of occurrence.\n",
        "\n",
        "* The logistic function (also known as the sigmoid function) convert the probabilities into binary values which could be further used for predictions.\n",
        "\n",
        "#### Why Logistic, not Linear?\n",
        "\n",
        "With binary classification, let ‚ÄòX‚Äô be some feature and ‚Äòy‚Äô be the output which can be either 0 or 1.\n",
        "\n",
        "\n",
        "For Linear Regression, we had the hypothesis $\\hat{y} = w.X + b $ , whose output range was the set of all Real Numbers.\n",
        "\n",
        "Now, for Logistic Regression our hypothesis is  $h_{\\theta}(X) = sigmoid(w.X +b)$ , whose output range is between 0 and 1 because by applying a sigmoid function, we always output a number between 0 and 1.  \n",
        "\n",
        "$h_{\\theta}(X) = \\frac{1}{1 +e^{-(w.X + b)}}$\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/logistic.png\" width= 400px/>\n",
        "</center>\n",
        "\n",
        "The probability that the output is 1 given its input can be represented as:\n",
        "\n",
        "$ P(y = 1 \\ | \\ x)$\n",
        "\n",
        "\n",
        "In the formula of the logistic model,\n",
        "\n",
        "when $ w.X + b == 0 $, then the probability (p) will be 0.5,\n",
        "\n",
        "similarly, $ w.X + b > 0 $, then the p will be going towards 1 and $ w.X + b < 0 $, then the probability (p) will be going towards 0.\n",
        "\n",
        "Interpretation of the weights differ from the Linear Regression as the output of the Logistic Regression is in probabilities between 0 and 1. Instead of the slope co-efficient(w) being the rate of change of the p as X changes, now the slope co-efficient is interpreted as the rate of change of the ‚Äúlog odds‚Äù or \"logit function\" as X changes.\n",
        "\n",
        "#### Logit Function \n",
        "\n",
        "Logistic regression can be expressed as:\n",
        "\n",
        "$log (\\frac{p}{1 - p}) = w.X + b$\n",
        "\n",
        "where, the left hand side is called the logit or log-odds function, and $\\frac{p}{1 - p}$ is called odds.\n",
        "\n",
        "The odds signifies the ratio of probability of success to probability of failure. Therefore, in Logistic Regression, linear combination of inputs are mapped to the log(odds) - the output being equal to 1.\n",
        "\n",
        "If we take an inverse of the above function, we get:\n",
        "\n",
        "$p(X) = \\frac{e^{w.X + b}}{1 +e^{w.X + b}}$\n",
        "\n",
        "This is known as the Sigmoid function and it gives an S-shaped curve as shown above. It always gives a value of probability ranging from 0 < p < 1.\n",
        "\n",
        "#### Cost Function of the Logistic Regression\n",
        "\n",
        "The cost function consists of parameters/weights, when we say we want to optimize a cost function by this we simply refer to finding the best values of the parameters/weights.\n",
        "\n",
        "Instead of Mean Squared Error, we use a cost function called Binary Cross-Entropy, also known as Log Loss. Binary Cross-entropy loss can be divided into two separate cost functions: one for ùë¶=1 and one for ùë¶=0, that is, when the hypothesis function predicts whether the patient has heart disease or not. The Cost function is defined as follows:\n",
        "\n",
        "\n",
        "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} Cost(h_\\theta(x^{(i)}), y^{(i)}) $\n",
        "\n",
        "\n",
        "$ Cost(h_\\theta(x), y) = -log(h_\\theta(x)) $      if $y = 1$\n",
        "\n",
        "$ Cost(h_\\theta(x), y) = -log(1 - h_\\theta(x)) $      if $y = 0$\n",
        "\n",
        "\n",
        "Our task now is to choose the best parameters `w` in the above function, given the current training set, in order to minimize errors. \n",
        "\n",
        "The procedure is similar to what we did for linear regression: define a cost function and try to find the best possible values of each `w` by minimizing the cost function output. The minimization will be performed by a gradient descent algorithm, whose task is to parse the cost function output until it finds the lowest minimum point.\n",
        "\n",
        "**Above functions compressed into one**\n",
        "\n",
        "$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - (h_\\theta(x^{(i)}))] $\n",
        "\n",
        "Multiplying by $ùë¶$ and $(1‚àíùë¶)$ in the above equation is tricky, let‚Äôs us use the same equation to solve for both $y=1$ and $y=0$ cases. If $y=0$, the first side cancels out. If $y=1$, the second side cancels out. In both cases we only perform the operation we need to perform.\n",
        "\n",
        "\n",
        "**Vectorized cost function**\n",
        "\n",
        "$ h = sigmoid(w.X +b) $\n",
        "\n",
        "$J(\\theta) =  \\frac{1}{m} . [-y^{T} log(h) - (1 - y)^{T} log (1-h)]$\n",
        "\n",
        "You must be wondering why there is a negative(-) sign in the cost function,\n",
        "if you see,the values present in the log will be probabilities between 0 and 1, so,the value of log1 is 0 and the value of log0 is negative(-) infinity.\n",
        "So,the values from the cost function will always be in negative terms and that is why we add negative(-) sign to it.\n",
        "\n",
        "#### Gradient Descent\n",
        "\n",
        "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function (cost). It is just the derivative of the loss function with respect to its weights.\n",
        "\n",
        "We get the below equation after finding the derivative of the loss function:\n",
        "\n",
        "$\\frac{\\partial J(\\theta)}{\\partial \\theta} =  \\frac{1}{m} . (h - y) X^{T} $\n",
        "\n",
        "\n",
        "**Note:** To minimize the cost function we use Gradient Descent refer to the following [link](https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17) for the derivation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYydQX1t-Gb"
      },
      "source": [
        "#### Implementation of Logistic Regression\n",
        "\n",
        "For more details of the logstic regression refer to the following [link](https://medium.com/analytics-vidhya/understanding-logistic-regression-b3c672deac04)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRJTI5_3Ffg5"
      },
      "source": [
        "# Intialize the weights for the given data\n",
        "intercept = np.ones((X_train.shape[0], 1))  # Total number of samples\n",
        "x_train = np.concatenate((intercept, X_train), axis=1)\n",
        "weight = np.zeros(x_train.shape[1]) # Total number of features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Utx3koxFl6V"
      },
      "source": [
        "weight.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqYTQqhOQe4G"
      },
      "source": [
        "# Define Sigmoid Function\n",
        "def sigmoid(x, weight):\n",
        "    \n",
        "    # YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8AMR-DHC4wS"
      },
      "source": [
        "# Function to calculate the Binary cross entropy loss\n",
        "def cost_function(y_hat, y):\n",
        "    \n",
        "    # YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DE2067jF29o"
      },
      "source": [
        "# Iterate over training data to update the weights using gradient descent\n",
        "# lr -> learning rate\n",
        "def fit(x, y, weight, lr, iterations):\n",
        "    for i in range(iterations):\n",
        "        \n",
        "        # Apply the sigmoid function \n",
        "        y_hat = sigmoid(x, weight)\n",
        "\n",
        "        # Compute the loss \n",
        "        loss = cost_function(y_hat, y)\n",
        "        \n",
        "        # Gradient Calculation\n",
        "        # The Gradient descent is just the derivative of the cost function with respect to its weights\n",
        "        dW = # YOUR CODE HERE\n",
        "\n",
        "        # The weights are updated by subtracting the derivative (gradient descent) times the learning rate\n",
        "        # Gradient gives us the direction of the steepest ascent of the cost function and the direction of steepest descent \n",
        "        # is opposite to the gradient and that is why we substract the gradient from the weights\n",
        "        # Updating the weights\n",
        "        weight -= # YOUR CODE HERE\n",
        "\n",
        "    return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9LevYTLGIuc"
      },
      "source": [
        "# Starting the timer\n",
        "t0 = time()\n",
        "\n",
        "# Call the 'fit' function\n",
        "# The learning rate is usually a small number to control how fast or slow we want to move towards minimization\n",
        "updated_weights = fit(x_train, y_train, weight, 0.1, 500)\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cL3dXklGTka"
      },
      "source": [
        "# Function to predict the class label using test data\n",
        "def predict(x_new , weight, threshold):\n",
        "    x_new = np.concatenate((np.ones((x_new.shape[0], 1)), x_new), axis=1)  \n",
        "    result = sigmoid(x_new, weight)\n",
        "    result = # YOUR CODE HERE: Set the threshold\n",
        "    y_predictions = np.zeros(result.shape[0])\n",
        "    for i in range(len(y_predictions)):\n",
        "        if result[i] == True: \n",
        "            y_predictions[i] = 1\n",
        "        else:\n",
        "            continue\n",
        "    return y_predictions "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIJ5TjLUGa7w"
      },
      "source": [
        "# Call the function by passing the test data and updated weights and threshold value\n",
        "# YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGY_iP3WGhbW"
      },
      "source": [
        "# Accuracy on test data\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbJC__VwQgjj"
      },
      "source": [
        "### Apply Logistic Regression from sklearn\n",
        "\n",
        "Refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for Logistic Regression from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxRi2fPsYlPn"
      },
      "source": [
        "# Create an instance for logistic regression\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Starting the timer\n",
        "t0 = time()\n",
        "\n",
        "# Train the model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IzNXmQtIo1Y"
      },
      "source": [
        "# Get the predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIV0q8wLJYm0"
      },
      "source": [
        "# For retrieving the slope use 'log_reg.coef_'\n",
        "print('Coefficients: ', log_reg.coef_)\n",
        "\n",
        "# To retrieve the intercept\n",
        "print('Intercept: ', log_reg.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUQAr-kQZAvy"
      },
      "source": [
        "# Storing the actuals and predictions in a dictionary to see the results\n",
        "actual = []\n",
        "predcition = []\n",
        "\n",
        "for i,j in zip(y_test,y_pred):\n",
        "  actual.append(i)\n",
        "  predcition.append(j)\n",
        "  \n",
        "dic = {'Actual':actual,\n",
        "       'Prediction':predcition\n",
        "       }\n",
        "result  = pd.DataFrame(dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SivZiG4OZTMe"
      },
      "source": [
        "result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hh2rvUFZY_9"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjNThs1ZcQ1"
      },
      "source": [
        "# Calculate the accuracy\n",
        "# YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBPCoDkZ_1O"
      },
      "source": [
        "# Print the classification report\n",
        "# YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAV45N4KaFm8"
      },
      "source": [
        "# Confusion matrix\n",
        "# YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4MWqbUvnPpO"
      },
      "source": [
        "### Problem Statement: Multi-Layer Perceptron (MLP) Implementation using Keras on MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJ-Ng1Ywp_X"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "\n",
        "\n",
        "### Description\n",
        "\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, \n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately). \n",
        "2. Each image is Size Normalized and Centered \n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
        "\n",
        "\n",
        "### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90‚Äôs) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Q5lFwLoFZy"
      },
      "source": [
        "### Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afvcId2GMW2t"
      },
      "source": [
        "# From keras datasets download mnist images dataset\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ3JC110oMkC"
      },
      "source": [
        "Here dataset is loaded and divided into train and test images and corresponding labels. MNIST comes with 70,000 data samples with 60,000 being training data and 10,000 being test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8HKY_s3oQiQ"
      },
      "source": [
        "# Check for the shape of images and labels\n",
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX9VV9V4oeCi"
      },
      "source": [
        "### Visualize the MNIST Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtEhgeV8obzz"
      },
      "source": [
        "# YOUR CODE HERE: Plot the images from the dataset and reshape to 28x28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZoz49Dvp9T8"
      },
      "source": [
        "### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXIA3ELurUUV"
      },
      "source": [
        "# Each image in the MNIST dataset is represented as a 28x28x1 (height x width x gray scale image) image\n",
        "# but in order to apply a neural network we must\n",
        "# first \"flatten\" the image to be a list of 28x28=784 pixels\n",
        "# Reshape the train and test images\n",
        "train_images = train_images.reshape((train_images.shape[0], 28 * 28 * 1)) # 60000, 784\n",
        "test_images = test_images.reshape((test_images.shape[0], 28 * 28 * 1))    # 10000, 784"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALDfwFwqu72"
      },
      "source": [
        "# Check for the shape after flattening the images\n",
        "train_images.shape, test_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr_MQrE6sIAm"
      },
      "source": [
        "# Scale the pixels of images to the range of [0, 1]\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fruPWvxGqyIi"
      },
      "source": [
        "# 'to_categorical' converts a class vector (integers) to binary class matrix. E.g. for use with categorical_crossentropy. \n",
        "# y (labels): class vector to be converted into a matrix (integers from 0 to num_classes) to one-hot vector.\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels)\n",
        "test_labels = np_utils.to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyo6tdeAzR03"
      },
      "source": [
        "Each pixel in the MNIST dataset has an integer label in the range [0, 9], one for each of the possible ten digits in the MNIST dataset. A label with a value of 0 indicates that the corresponding image contains a zero digit. Similarly, a label with a value of 8 indicates that the corresponding image contains the number eight.\n",
        "\n",
        "However, we first need to transform these integer labels into vector labels, where the index in the vector for label is set to 1 and 0 otherwise (this process is called one-hot encoding).\n",
        "\n",
        "For example, consider the label 3 and we want to binarize/one-hot encode it ‚Äî the label 3 now becomes `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`. Notice how only the index for the digit three is set to one ‚Äî all other entries in the vector are set to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddM18-eCyIz2"
      },
      "source": [
        "### The Multilayer Perceptron and Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-qja66yQo9"
      },
      "source": [
        "An MLP is composed of \n",
        "\n",
        "* one (passthrough) **input layer**, \n",
        "* one or more layers **hidden layers**, and \n",
        "* one final layer called the **output layer** as shown in the figure below. \n",
        "\n",
        "The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a **bias neuron** and is fully connected to the next layer.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1007.png\" width= 500px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "MLPs are trained using **backpropagation training algorithm**.\n",
        "\n",
        "In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network‚Äôs error with regard to every single model parameter. \n",
        "\n",
        "In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
        "\n",
        "Let‚Äôs run through this algorithm in detail:\n",
        "\n",
        "* It handles one mini-batch at a time (say, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an **epoch**.\n",
        "\n",
        "* Each mini-batch is passed to the network‚Äôs **input layer**, which sends it to the first **hidden layer**. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the **output layer**. This is the **forward pass**: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
        "\n",
        "* Next, the algorithm measures the network‚Äôs output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
        "\n",
        "* Then it computes how much each output connection contributed to the error.\n",
        "This is done analytically by applying the chain rule, which makes this step fast and precise.\n",
        "\n",
        "* The algorithm then measures how much of these error contributions came from\n",
        "each connection in the layer below, again using the chain rule, working backward\n",
        "until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n",
        "\n",
        "* Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
        "\n",
        "Let's summarize this algorithm again: for each training instance, the backpropagation algorithm first makes a prediction (**forward pass**) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (**reverse pass**), and finally tweaks the connection weights to reduce the error (Gradient Descent step).\n",
        "\n",
        "In order for this algorithm to work properly, the step function was replaced with an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1-Qn_6F0rtb"
      },
      "source": [
        "### Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwDHoCd0tAE"
      },
      "source": [
        "Some of the activation functions are shown below:\n",
        "\n",
        "* **Logistic (sigmoid) function:**\n",
        "\n",
        "$$œÉ(z) = \\frac{1}{1 + exp(‚Äìz)}$$\n",
        "\n",
        "It is an S-shaped function, exists between $0$ to $1$. Therefore, it is especially used for models where we have to predict the probability as an output. The function is differentiable.\n",
        "That means, we can find the slope of the sigmoid curve at any two points.\n",
        "\n",
        "* **Hyperbolic tangent function:** \n",
        "\n",
        "$$tanh(z) = 2œÉ(2z) ‚Äì 1 = \\frac{2}{1 + exp(‚Äì2z)} - 1$$\n",
        "\n",
        "Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from $‚Äì1$ to $1$. That range tends to make each layer‚Äôs output more or less centered around $0$ at the beginning of training, which often helps speed up convergence.\n",
        "\n",
        "* **Rectified Linear Unit function:**\n",
        "\n",
        "$$ReLU(z) = max(0, z)$$\n",
        "\n",
        "The ReLU function is continuous but unfortunately not differentiable at $z = 0$\n",
        "(the slope changes abruptly, which can make Gradient Descent bounce around),\n",
        "and its derivative is $0$ for $z < 0$. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n",
        "\n",
        "These popular activation functions and their derivatives are represented in\n",
        "the figure below. \n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1008.png\" width=700px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**Why do we need activation functions?** \n",
        "\n",
        "If we chain several linear transformations, all we get is a linear transformation. For example, if $f(x) = 2x + 3$ and $g(x) = 5x ‚Äì 1$, then chaining these two linear functions gives you another linear function: $f(g(x)) = 2(5x ‚Äì 1) + 3 = 10x + 1.$ \n",
        "\n",
        "So if we don‚Äôt have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and we can‚Äôt solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xumlwPO12OX_"
      },
      "source": [
        "### MLP Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW-vaO362PyH"
      },
      "source": [
        "MLPs can be used for classification and regression tasks. In classification, they can perform (i) Binary Classification (ii) Multilabel Binary Classification, and (iii) Multiclass classification\n",
        "\n",
        "* **Binary classification:** Used when there are only two distinct classes and the data we want to classify belongs exclusively to one of those classes, e.g. classifying if a review sentiment is positive or negative.\n",
        "\n",
        "* **Multilabel binary classification:** Used when there are two or more classes and the data we want to classify belongs to none of the classes or all of them at the same time, e.g. classifying which traffic signs are shown in an image.\n",
        "\n",
        "  Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels\n",
        "\n",
        "* **Multiclass classification:** Used when there are three or more classes and the data we want to classify belongs exclusively to one of those classes, e.g.  out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), we need to have one output neuron per class, and we should use the **softmax activation function** for the whole output layer as shown in the figure below. The softmax function will ensure that all the estimated probabilities are between $0$ and $1$ and that they add up to $1$.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1009.png\" width=500px/>\n",
        "</center>\n",
        "\n",
        "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn1OPbo32qvM"
      },
      "source": [
        "### Building an Image Classifier Using the Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1byy_Tk6gMp"
      },
      "source": [
        "\n",
        "\n",
        "The Sequential function initializes a linear stack of layers which allows to add more layers later using the Dense module.\n",
        "\n",
        "**Dense** layers are \"fully connected\" layers. In a dense layer, all nodes in the previous layer connect to the nodes in the current layer.\n",
        "\n",
        "\n",
        "**Sequential model** is the easiest way to build a model in Keras. It allows us to build a model layer by layer. Each layer has weights that correspond to the layer that follows it. This is called the Sequential API.  We use the ‚Äòadd()‚Äô function to add layers to our model. We will add two layers and an output layer.\n",
        "\n",
        "For more details on Sequential API refer to the following [Documentation](https://keras.io/models/sequential/\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voL_-8zd6Voe"
      },
      "source": [
        "from keras.layers import Dense, Flatten # Dense layers are \"fully connected\" layers\n",
        "from keras.models import Sequential # Importing sequential model\n",
        "\n",
        "# Flatten the image dimension to pass through the network\n",
        "image_size = 784 # 28*28\n",
        "\n",
        "# Create an instance for the sequential model\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOMnqD2x4vfB"
      },
      "source": [
        "#### Building the Neural network using Sequential API\n",
        "\n",
        "Here is a classification MLP with two hidden layers:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhGBvZVi7UzB"
      },
      "source": [
        "# Create model with 2 hidden layers and one output layer\n",
        "\n",
        "model.add(Dense(256, activation='relu', input_shape=(image_size,))) # first fully connected layer in the network\n",
        "\n",
        "# second fully connected layer in the network\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# The last layer is the output layer. It only has one node, which is for our prediction.\n",
        "# Output layer - 10 output classes,  softmax activation to obtain the normalized class probabilities for each prediction.\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GNcpB5h5rE8"
      },
      "source": [
        "Let‚Äôs go through the above code line by line:\n",
        "\n",
        "* We first Flatten the image dimension whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). We can specify it using the 'input_shape' parameter while adding the first Dense hidden layer with 256 neurons.\n",
        "\n",
        "* Next, it will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes $h_{W, b}(X) = œï( XW + b)$.\n",
        "\n",
        "* Then we add a second Dense hidden layer with 128 neurons, also using the ReLU\n",
        "activation function.\n",
        "\n",
        "* Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive). Softmax is a sigmoid function applied to an independent variable with more than two categories.\n",
        "\n",
        "* Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which has a higher probability.\n",
        "\n",
        "Instead of adding the layers one by one we can pass a list of layers when creating the Sequential model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfmXSqDV3hG6"
      },
      "source": [
        "# Summary of the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjPh9rX16YeZ"
      },
      "source": [
        "The model‚Äôs `summary()` method displays all the model‚Äôs layers, including each layer‚Äôs name, its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV5CrsfyOlTT"
      },
      "source": [
        "#### Compiling the model\n",
        " \n",
        "After a model is created, we must call its compile() method to specify the loss function and the optimizer to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRKnKO-g9MRg"
      },
      "source": [
        "# Compile model\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh57ACKK7L2r"
      },
      "source": [
        "In the above code cell, \n",
        "\n",
        "* first, we use the **\"categorical_crossentropy\"** loss because we have more than two label classes. Here, we have one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3). If we were doing binary classification (with one or more binary labels), then we would use the \"sigmoid\" activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n",
        "\n",
        "* Regarding the optimizer, **\"sgd\"** means that we will train the model using simple Stochastic Gradient Descent. \n",
        "\n",
        "* Finally, since this is a classifier, it‚Äôs useful to measure its **\"accuracy\"** during training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hov8hLS59eBn"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3knzaM9bVl"
      },
      "source": [
        "# Training model on Training set\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgcTEOBI9rus"
      },
      "source": [
        "The `fit()` method returns a History object containing the training parameters\n",
        "(`history.params`), the list of epochs it went through (`history.epoch`), and most importantly a dictionary (`history.history`) containing the loss and extra metrics it measured at the end of each epoch on the training set. Keras does backpropagation automatically by using the fit method\n",
        "\n",
        "Let's plot the learning curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MRPUY7193oJ"
      },
      "source": [
        "# Visualize training metrics\n",
        "df = pd.DataFrame(history.history)\n",
        "df.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "\n",
        "# Set the vertical range to [0-1]\n",
        "plt.gca().set_ylim(0, 1) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLu0EM9niNka"
      },
      "source": [
        "We can see that both the training accuracy and the validation accuracy steadily\n",
        "increase during training, while the training loss and the validation loss decrease. Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting.\n",
        "\n",
        "Once we are satisfied with the model‚Äôs validation accuracy, we should evaluate it on the test set to estimate the generalization error before we deploy the model to production. We can easily do this using the `evaluate()` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhnsd3jGEH4a"
      },
      "source": [
        "#### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMxqDWtI9hQp"
      },
      "source": [
        "# Model performance on test set\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrcf0U-vgueY"
      },
      "source": [
        "Let's see how to implement back propagation algorithm from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJt-uGJvymM2"
      },
      "source": [
        "### Problem Statement: Implementing the XOR Gate using Backpropagation in Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u19_H2cqaF22"
      },
      "source": [
        "####  X-OR problem in Neural Networks\n",
        "\n",
        "The XOR problem is a classic problem in artificial neural network research. It consists of predicting output value of exclusive-OR gate. We do it using a Multi layer perceptron. \n",
        "\n",
        "Minksy and Papert (1969) showed that classifying XOR was a big problem for neural network architectures of the 1960s, known as perceptrons. Let‚Äôs looks at how a multi-layer perceptron based neural network solves this problem of the truth table, called the ‚ÄòXOR‚Äô (either A or B but not both). \n",
        "\n",
        "\n",
        "Implementing logic gates using neural networks help understand the mathematical computation by which a neural network processes its inputs to arrive at a certain output. This neural network will deal with the XOR logic problem. An XOR (exclusive OR gate) is a digital logic gate that gives a true output only when both its inputs differ from each other. The truth table for an XOR gate is shown below:\n",
        "\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcR9Q-xKcyvxJ13ElQFJsBTws8coBQeZen2qpl0x0XQqkN0DnYRc)\n",
        "\n",
        "\n",
        "The goal of the neural network is to classify the input patterns according to the above truth table. If the input patterns are plotted according to their outputs, it is seen that these points are not linearly separable. Hence the neural network has to be modeled to separate these input patterns using decision planes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgiBwDDnaRqk"
      },
      "source": [
        "Here, we build a neural network which consist of one input layer with two nodes (X1,X2); one hidden layer with two nodes (since two decision planes are needed); and one output layer with one node (Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPLLaOx_aVO6"
      },
      "source": [
        "#### Define the inputs and structure of the neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeZm36ZOF5rh"
      },
      "source": [
        "# These are XOR inputs\n",
        "x = np.array([[0,0,1,1],[0,1,0,1]])\n",
        "\n",
        "# These are XOR outputs\n",
        "y = np.array([[0,1,1,0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjGampQpaFK_"
      },
      "source": [
        "# Number of inputs\n",
        "n_x = 2\n",
        "\n",
        "# Number of neurons in output layer\n",
        "n_y = 1\n",
        "\n",
        "# Number of neurons in hidden layer\n",
        "n_h = 2\n",
        "\n",
        "# Total training examples\n",
        "m = x.shape[1]\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.5\n",
        "\n",
        "# Define random seed for consistent results\n",
        "np.random.seed(2)\n",
        "\n",
        "# Define weight matrices for neural network\n",
        "# YOUR CODE HERE\n",
        "\n",
        "losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B-hoQtcvuac"
      },
      "source": [
        "#### Update the weights using feedforward and backpropagation algorithm\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com//DLFA/Experiment_related_data/XOR.png\" width=700px/>\n",
        "</center>\n",
        "\n",
        "For simplicity we consider '$W_{11}$', '$W_{12}$', '$W_{21}$' and '$W_{22}$' as weight vector '$W_{1}$'. Weights '$W_{3}$' and '$W_{4}$' as '$W_{2}$'\n",
        "\n",
        "Here 'Z' is the dot product of weight vector 'W' and the input vector 'X'. Again we vectorize '$Z_{11}$', '$Z_{12}$' as '$Z_{1}$' and '$Z_{2}$' remains same\n",
        "\n",
        "'A' is the activation function. We used sigmoid activation function in our network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik9tBgpXKmGK"
      },
      "source": [
        "#### Forward propagation\n",
        "\n",
        "In the forward propagation all 'Z' and 'A' will be calculated until we get to the end of our network giving us our prediction 'Y'.\n",
        "\n",
        "$ Z_{1} = W_1 . X + b_1$     --------> 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV60omIIKfQ5"
      },
      "source": [
        "#### Activation Function\n",
        "\n",
        "Sigmoid function is a S-shaped curve which predicts the probability as an ouput.\n",
        "\n",
        "$A_1 = \\frac{1}{1 + e ^ {- Z_1}}$   ---------> 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9dDvkKLuFa"
      },
      "source": [
        "$ Z_{2} = W_2 . A_1 + b_2$    ---------> 3\n",
        "\n",
        "Y = $A_2 = \\frac{1}{1 + e ^ {- Z_2}}$   ---------> 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwXEuxUTbNe9"
      },
      "source": [
        "# Sigmoid Function\n",
        "def sigmoid(z):\n",
        "    # YOUR CODE HERE:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t3jbYr-GtEO"
      },
      "source": [
        "# Forward propagation\n",
        "def forward_prop(w1,w2,x):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return z1,a1,z2,a2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLO4uCjAyEXd"
      },
      "source": [
        "Basic Algorithm which explains feedforward and backpropagation\n",
        "\n",
        "1. Inputs are taken\n",
        "2. The weights are usually randomly selected.\n",
        "3. Calculate the output of every neuron from the input layer, to the hidden layers and to the output layer.\n",
        "4. Apply sigmoid function at hidden and output layers.\n",
        "5. Calculate the error in the outputs\n",
        "\n",
        "    *Error = Predicted Output ‚Äì Actual Output*\n",
        "5. Travel back from the output layer to the hidden layer to adjust the weights such that the error is minimized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woRC6ENkhkQo"
      },
      "source": [
        "**Chain rule to find the change in error with respect to W2 (Weights):**\n",
        "\n",
        "$\\frac{\\partial E}{\\partial W_{2}} = \\frac{\\partial E}{\\partial O} \\cdot \\frac{\\partial O}{\\partial Z_{2}} \\cdot \\frac{\\partial Z_{2}}{\\partial W_{2}}$\n",
        "\n",
        "* **Change in error with respect to output**\n",
        "\n",
        "  Suppose the actual output is represented as 'O' and the predicted output is represented as 'a2', then the error would be calculated as:\n",
        "\n",
        "  $E = \\frac{1}{2}\\left ( a2-O \\right )^{2}$\n",
        "\n",
        "  Differentiate the error with respect to the output\n",
        "\n",
        "  $\\frac{\\partial E}{\\partial O} = -\\left (a2-O  \\right )$\n",
        "\n",
        "   $\\frac{\\partial E}{\\partial O} = O-a2$\n",
        "\n",
        "  In the code, predicted output is represented as output_layer_outputs and the actual output is represented as y\n",
        "\n",
        "* **Change in output with respect to Z2** \n",
        "\n",
        "  Thus, $\\frac{\\partial O}{\\partial Z_{2}}$ is effectively the derivative of Sigmoid\n",
        "\n",
        "  $f\\left ( z \\right ) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "  $f'(z) = (1+e^{-z})-1[1-(1+e^{-z})-1]$\n",
        "\n",
        "  ${f}'\\left ( z \\right ) = sigmoid(z)(1-sigmoid(z))$\n",
        "\n",
        "  $\\frac{\\partial O}{\\partial Z_{2}} = (a2)(1-a2)$ \n",
        "\n",
        "\n",
        "* **Change in $Z_2$ with respect to $W_2$ (Weights)**\n",
        "\n",
        "  $Z_2 = W^T.A_1 $\n",
        "\n",
        "  On differentiating $Z_2$ with respect to $W_2$, we will get the value $A_1$ itself\n",
        "\n",
        "  $\\frac{\\partial Z_{2}}{\\partial W_{2} }$ = $A_1$ where $A_1$ = $sigmoid(Z_{1})$\n",
        "\n",
        "**Chain rule to find the change in error with respect to W2 (Weights):**\n",
        "\n",
        "$\\frac{\\partial E}{\\partial W_{2}} = \\frac{\\partial E}{\\partial O} \\cdot \\frac{\\partial O}{\\partial Z_{2}} \\cdot \\frac{\\partial Z_{2}}{\\partial W_{2}}$\n",
        "\n",
        "$\\frac{\\partial E}{\\partial W_{2}} = (a2-y) . a2(1-a2). A_1$\n",
        "\n",
        "In the code $A_1$ is represented as $a_1$\n",
        "\n",
        "**Update the weights using Gradient Descent**\n",
        "\n",
        "$W_{new} = W_{old} - lr * \\frac{\\partial E}{\\partial W_{2}} $\n",
        "\n",
        "In the code, $W_{old}$ is represented as $w_2$,  $lr$ is represented as learning rate and $\\frac{\\partial E}{\\partial W}$ is represented as $W_2$.\n",
        "\n",
        "Similarly, W1 (Weights) are updated using chain rule.\n",
        "\n",
        "**Note:** For more details refer to the following [link](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC_1em2QZ6aj"
      },
      "source": [
        "# Backward propagation\n",
        "def back_prop(w1,w2,z1,a1,z2,a2,y):\n",
        "\n",
        "    # Updating the weights with respect to w2 (hidden weights)\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Updating the weights with respect to w1 (input weights)\n",
        "\n",
        "    dz1 = np.dot(w2.T,dz2) * a1*(1-a1)\n",
        "    dw1 = np.dot(dz1,x.T)\n",
        "   \n",
        "    return dz2, dw2, dz1, dw1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FRKJCn5aXpv"
      },
      "source": [
        "# Training the network\n",
        "iterations = 10000\n",
        "for i in range(iterations):\n",
        "\n",
        "    # Call the forward propagation function\n",
        "    z1, a1, z2, a2 = forward_prop(w1, w2, x)\n",
        "\n",
        "    # Binary cross entropy loss \n",
        "    # YOUR CODE HERE\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Call the Backward propagation function\n",
        "    dz2, dw2, dz1, dw1 = back_prop(w1, w2, z1, a1, z2, a2, y)\n",
        "    \n",
        "    # Updating weights using gradient descent\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "# We plot losses to see how our network is doing\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"EPOCHS\")\n",
        "plt.ylabel(\"Loss value\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEvQxsVtcw9Z"
      },
      "source": [
        "Define a predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q780Uw3cdfY"
      },
      "source": [
        "def predict(w1,w2,input):\n",
        "    z1,a1,z2,a2 = forward_prop(w1,w2,test)\n",
        "    # Set the threshold for the predicted output\n",
        "    if a2 >= 0.5:\n",
        "        print(\"For input\", [i[0] for i in input], \"output is 1\")\n",
        "    else:\n",
        "        print(\"For input\", [i[0] for i in input], \"output is 0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HQIyb3FcuKc"
      },
      "source": [
        "Here are the predictions of our trained neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rkf6P4Qcp7U"
      },
      "source": [
        "# YOUR CODE HERE: To call the predict function to get the predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlW2FdY5_JK"
      },
      "source": [
        "Our goal with backpropagation is to update each of the weights in the network so that predicted output will be closer to the actual output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Suppose the network has 784 inputs, 32 neurons in the first hidden layer, 16 neurons in the second hidden layer, and 10 neurons in the output layer. How many parameters (weights) does the network have?\n",
        "Answer1 = \"\" #@param [\"\",\"25248\", \"25600\",\"672\",\"25760\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRvN7yU-e18h"
      },
      "source": [
        "#### Consider the below figure and answer Q.2.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Picture.png\" width=400px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. You are given the above neural networks which take two binary valued inputs x1, x2 ‚àà {0, 1} and the activation function is the threshold function (h(x) = 1 if x > 0; 0 otherwise). Which of the following logical functions does it compute?\n",
        "Answer2 = \"\" #@param [\"\",\"AND\", \"OR\",\" NAND\",\"XOR\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}