{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of M3_AST_27_Convolutional_Neural_Networks_for_Sentence_Classification_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnshcodie/IIScEx_2021/blob/main/Copy_of_M3_AST_27_Convolutional_Neural_Networks_for_Sentence_Classification_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 27: Natural Language Processing - II (Convolutional Neural Networks for Sentence Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        " \n",
        "*  represent words in the sentences using pretrained word embeddings\n",
        "*  generate vector representation of words in the sentence using Glove\n",
        "*  build a convolutional Neural Network for Sentence Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtOp3EVPL-0o"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "#### Description\n",
        "\n",
        "\n",
        "The SICK(Sentences Involving Compositional Knowledge) dataset contains 9840 English sentence pairs:\n",
        "\n",
        "- two sentences, SentenceA and SentenceB\n",
        "- their relation to each other from SentenceA to SentenceB and from SentenceB to SentenceA (entailment, contradiction, neutral)\n",
        "\n",
        "For more details about the dataset refer to the following [link](https://zenodo.org/record/2787612#.YbA5pCbhU8o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C-eklopT0q0"
      },
      "source": [
        "### Problem Statement\n",
        "\n",
        "The aim of this assignment is to study the use of a Siamese neural network for solving the textual\n",
        "entailment problem. We will use the CNN based\n",
        "architecture for sentence representation.\n",
        "\n",
        "Every instance of this dataset consists of two sentences (A and B) and the information whether the sentence (A/B)\n",
        "contradicts/neutral/entails the sentence (B/A). We treat this as a seven-category classification problem\n",
        "and design a Siamese neural network for solving this classification problem. Note that the input to\n",
        "the Siamese network will be two sentences and the output could be a 7-dimensional vector for the\n",
        "7 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_27_Convolutional_Neural_Networks_for_Sentence_Classification_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\") \n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\")\n",
        "    ipython.magic(\"sx unzip glove.6B.zip\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/sententence_data.csv\")\n",
        "\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVM_u_Czx_jZ"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjtacz2UCmuX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, concatenate, Dense, Bidirectional, Dropout, Flatten, Conv1D, MaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyie_w3yEq_"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2fGPg1t4_ZP"
      },
      "source": [
        "def load_data(dataset):\n",
        "  data = pd.read_csv(dataset)\n",
        "  sentence_data = data.drop('pair_ID', axis=1)\n",
        "  return sentence_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSFQA3ic5Y4o"
      },
      "source": [
        "# Load the dataset\n",
        "sentence_data = load_data('sententence_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgxfwLT55b8-"
      },
      "source": [
        "# Print the first 5 rows from the data\n",
        "sentence_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJTV_7H2CfjZ"
      },
      "source": [
        "# Combining the label columns (entailment_AB, entailment_BA) to create a final labels column\n",
        "sentence_data['labels'] = sentence_data['entailment_AB'].astype(str) + 'and' + sentence_data['entailment_BA'].astype(str)\n",
        "\n",
        "# No of unique labels after combining the entailment_AB and entailment_BA\n",
        "print(len(sentence_data['labels'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJkecxGYZPM"
      },
      "source": [
        "# check for the unique label combinations \n",
        "print(np.unique(sentence_data['labels']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BQlbkoZDfGO"
      },
      "source": [
        "# checking for the output counts \n",
        "sentence_data['labels'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RbrsSj2yJAt"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0HUwo3qRhJo"
      },
      "source": [
        "# Converting the labels from categorical to numerical\n",
        "le = LabelEncoder()\n",
        "sentence_data['labels'] = le.fit_transform(sentence_data['labels'])\n",
        "sentence_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf_6eWLXyUus"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mOVR-b0_7Rp"
      },
      "source": [
        "def cleaning_dataset(df):\n",
        "    \n",
        "    # Pre-Processing\n",
        "    # converat all sentences to string format\n",
        "    df['sentence_A'] = df['sentence_A'].astype(str)\n",
        "    df['sentence_B'] = df['sentence_B'].astype(str)\n",
        "    \n",
        "    # convert all sentences to lower case\n",
        "    df['sentence_A'] = df['sentence_A'].apply(lambda sentence_A: sentence_A.lower())\n",
        "    df['sentence_B'] = df['sentence_B'].apply(lambda sentence_B: sentence_B.lower())\n",
        "       \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAFICGCh_-SR"
      },
      "source": [
        "sentence_data = cleaning_dataset(sentence_data)\n",
        "sentence_data = sentence_data.drop(['entailment_AB','entailment_BA'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JJ3n_8FAkU0"
      },
      "source": [
        "sentence_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLl2mdzBgEO"
      },
      "source": [
        "# Combining both the sentences as a corpus\n",
        "sentence_data['Sentences'] = sentence_data[['sentence_A', 'sentence_B']].apply(lambda x: str(x[0])+\" \"+str(x[1]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BbSPU936D0R"
      },
      "source": [
        "### Tokenize and Pad sequences\n",
        "\n",
        "A Neural Network only accepts numeric data, so we need to encode the reviews. Here use keras.Tokenizer() to encode the reviews into integers, where each unique word is automatically indexed (using `fit_on_texts` method) calculates the frequency of each word in our corpus/messages. \n",
        "\n",
        "`texts_to_sequences` method finally converts our array of sequences of strings to list of sequences of integers (most frequent word is assigned 1 and so on).\n",
        "\n",
        "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using `keras.preprocessing.sequence.pad_sequences.`\n",
        "\n",
        "`post`, pad or truncate the words in the back of a sentence\n",
        "`pre`, pad or truncate the words in front of a sentence\n",
        "\n",
        "Each word is assigned an integer and that integer is placed in a list. \n",
        "\n",
        "\n",
        "For example if we have a sentence “How text to sequence and padding works”. Each word is assigned a number. We suppose how = 1, text = 2, to = 3, sequence = 4, and = 5, padding = 6, works = 7. After texts_to_sequences is called our sentence will look like [1, 2, 3, 4, 5, 6, 7 ]. Now for suppose our MAX_SEQUENCE_LENGTH = 10. After padding our sentence will look like `pre` = [0, 0, 0, 1, 2, 3, 4, 5, 6, 7 ], `post` = [1, 2, 3, 4, 5, 6, 7, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzXMFeMbCtjp"
      },
      "source": [
        "# Tokenizer class from the keras.preprocessing.text module creates a word-to-index integer dictionary\n",
        "# Vectorize the text samples\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentence_data['Sentences'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-KVB44PDGkA"
      },
      "source": [
        "sen_seq_A = tokenizer.texts_to_sequences(sentence_data['sentence_A'])\n",
        "sen_seq_B = tokenizer.texts_to_sequences(sentence_data['sentence_B'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UymMrZNJDR2M"
      },
      "source": [
        "max_len = 50\n",
        "\n",
        "sen_seq_A = pad_sequences(sen_seq_A, maxlen=max_len, padding='post')\n",
        "sen_seq_B = pad_sequences(sen_seq_B, maxlen=max_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkg7rw3VEWsJ"
      },
      "source": [
        "print(sen_seq_A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpXRJXBRDgCW"
      },
      "source": [
        "print(sen_seq_A.shape, sen_seq_B.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMZ9Hb1l-gd"
      },
      "source": [
        "### Load the GloVe word embeddings\n",
        "\n",
        "**What is GloVe?**\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. These are essential for solving most Natural language processing problems.The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n",
        "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "Now, let us load the 50-dimensional GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNHXa8G4DMcm"
      },
      "source": [
        "embeddings_index = {}\n",
        "# Loading the 300-dimensional vector of the model\n",
        "f = open('/content/glove.6B.50d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUbiAQ1D5HI"
      },
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoOwxlsFEFdH"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQmtuo7MEIm4"
      },
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4P8bwYey5MD"
      },
      "source": [
        "### Splitting the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv6ilxXpqgSB"
      },
      "source": [
        "X = np.stack((sen_seq_A, sen_seq_B), axis=1)\n",
        "Y = sentence_data['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4doSO5lrTP5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.25, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW8SvDBF-oqt"
      },
      "source": [
        "# Check for the shape of train and test sets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4FP8JBWs5kk"
      },
      "source": [
        "# Storing the sentence_A and sentence_B data seperately for training\n",
        "train_s1 = X_train[:,0]\n",
        "train_s2 = X_train[:,1]\n",
        "test_s1 = X_test[:,0]\n",
        "test_s2 = X_test[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzlEd11evA_k"
      },
      "source": [
        "# Converting labels to array\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txmoVPF3S119"
      },
      "source": [
        "# one-hot encode the labels\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Juw0huFTD2A"
      },
      "source": [
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4wid6RBzVEs"
      },
      "source": [
        "### Define the Convolutional Neural Network (CNN) model\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/CNN_Sentence.png\" width=800px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "Deep learning models have achieved remarkable results in  computer  vision and speech recognition in recent years.  Within natural language processing,  much of the work with deep learning methods has involved learning word vector representations through neural language models and performing composition over the learned word vectors for classification .Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabularysize) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions.\n",
        "\n",
        "Convolutional neural networks (CNN) utilize layers  with  convolving filters that are  applied to local  features.   Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and  have  achieved  excellent results in semantic parsing, search query retrieval, sentence  modeling, and other traditional NLP tasks.\n",
        "\n",
        "In the present work, we train a simple CNN with one layer of convolution on top of word vectors obtained from  an  unsupervised neural language model. These vectors were trained on 100 billion words of Google News, and are publicly available. We initially keep the word vectors static and learn only the other parameters of the model.\n",
        "\n",
        "From the above figure we can observe that the inputs are words. Each word is represented by a vector of size (50, 100, 200, 300).\n",
        "Apply different filters on the word vectors to create convolutional feature map. Choose the maximum value of the result from each filter vector for pooled representation and then\n",
        "apply softmax to perform classification.\n",
        "\n",
        "\n",
        "We have seen the process by which one feature is extracted  from one filter. The  model uses multiple filters (with varying window sizes) to obtain multiple features. These features form the penultimate layer and are passed to a fully connected and then softmax is used to perform classification.\n",
        "\n",
        "#### 1-D Convolutions over text\n",
        "\n",
        "The application of convolutional neural networks is the same as in image data. The only difference is that 1D convolutions are applied instead of 2D convolutions. In images, the kernel slides in 2D but in sequence data like text data the kernel slides in one dimension. \n",
        "\n",
        "Let’s now take a look at how this CNN can be built. \n",
        "\n",
        "The convolution network will be made of of the following: \n",
        "\n",
        "* An embedding layer that turns the data into dense vectors of fixed size. \n",
        "\n",
        "* A `Conv1D` with no of filter units and the `relu` activation function.\n",
        "\n",
        "* A `MaxPooling1D` layer that downsamples the input by taking the maximum value. The pooling operation is used to combine the vectors resulting from different convolution windows into a single $l$-dimensional vector. This is done again by taking the max or the average value observed in resulting vector from the convolutions. Ideally this vector will capture the most relevant features of the sentence/document.\n",
        "\n",
        "* A `Dense` layer with hidden units for the fully connected layer.\n",
        "    \n",
        "* An output layer with the softmax activation function.\n",
        "\n",
        "\n",
        "\n",
        "**Note:** Refer to the following [link](https://cezannec.github.io/CNN_Text_Classification/) for more details of CNN in Text classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zx8Y4hlSngF"
      },
      "source": [
        "# Define a function which takes the input sequence and generate the vectors using the pretrained word embeddings\n",
        "# The pass it through the convolutional layers and then the max pooling layer\n",
        "def cnn_model(input_shape, vocab_size, embeddings, embedding_dim):\n",
        "  # Input as max sequence length\n",
        "  model_input = Input(shape=(input_shape,))\n",
        "\n",
        "  # Embedding layer\n",
        "  layer = Embedding(vocab_size, \n",
        "                 embedding_dim, \n",
        "                 weights=[embedding_matrix], \n",
        "                 input_length=max_len, \n",
        "                 trainable=False)(model_input)\n",
        "                 \n",
        "  # Convolutional layer              \n",
        "  layer = Conv1D(filters=16, kernel_size=3, activation='relu')(layer)\n",
        "  # MaxPool layer\n",
        "  layer = MaxPooling1D(pool_size=2)(layer)\n",
        "  output = Flatten()(layer)\n",
        "\n",
        "  model = Model(inputs=model_input, outputs=output)\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZgmRlisQqsQ"
      },
      "source": [
        "#### CNN for Sentence Classification\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/CNN_sentence_classification.png\" width=900px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "Here, we tokenize the sentences (sentence_A and sentence_B) into words and then map the text to sequences and for the input sequences we build the word embedding with the pretrained(word2vec, Glove, FastText) models.\n",
        "\n",
        "We pass the two sentence(A and B) to the CNN model which shares the same weight parameters, which provides the enocoded representation of the sentences. Then, we concatenate the output sentences(A and B) vectors pass it to the fully connected layers of neural networks to train the model and perform the classification at the final layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_uLufiSUbuE"
      },
      "source": [
        "# Call the CNN model with max sequence length, embeddings and word embedding dimensions\n",
        "cnn_model = cnn_model(max_len, vocab_size, embedding_matrix, embedding_dim)\n",
        "\n",
        "# sentence_A and sentence_B inputs as a max sequence length\n",
        "input_s1 = Input(shape=(max_len,),dtype='int32')\n",
        "input_s2 = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "# pass the input sentences to the CNN model which share the same weight parameters\n",
        "left_out = cnn_model(input_s1)\n",
        "right_out = cnn_model(input_s2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snqfQtB8eMO8"
      },
      "source": [
        "# Concatenate the sentence_A and sentence_B output vectors from the CNN model\n",
        "merge = concatenate([left_out, right_out], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Ce-9cpUpVw"
      },
      "source": [
        "# Fully connected layers\n",
        "sumx = Dense(256, activation='relu')(merge)\n",
        "sumx = Dense(128, activation='relu')(sumx)\n",
        "sumx = Dense(64, activation='relu')(sumx)\n",
        "sumx = Dropout(0.5)(sumx)\n",
        "\n",
        "# Output layer for 7 class classification\n",
        "pred = Dense(7, activation='softmax')(sumx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFuwU8tl1M57"
      },
      "source": [
        "# Creating the functional model with input sentences and outputs the 7 classes\n",
        "cnn_model = Model(inputs=[input_s1, input_s2], outputs=pred)\n",
        "cnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8wo9lrl1PKC"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t90gkKUfULWC"
      },
      "source": [
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training the CNN model\n",
        "history = cnn_model.fit([train_s1,train_s2],\n",
        "                    y_train, \n",
        "                    epochs=10,\n",
        "                    batch_size=16,\n",
        "                   validation_data=([test_s1, test_s2], y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Y3RDTfYUYX"
      },
      "source": [
        "# Get the predictions on the test set\n",
        "y_pred = cnn_model.predict([test_s1, test_s2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brpATi0fYuzu"
      },
      "source": [
        "yout = np.argmax(y_pred, axis=1)\n",
        "yout.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmzOFgfdYKuo"
      },
      "source": [
        "y_test = np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymUpEMhZBz_"
      },
      "source": [
        "yout.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-cCxjEqeFTP"
      },
      "source": [
        "print(y_test.shape, yout.shape)\n",
        "print(y_test[:5], yout[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. What does a 1D convolutional layer do in a CNN for Text Classification?\n",
        "Answer1 = \"\" #@param [\"\",\"It creates a convolution kernel that is convolved with the input layer over a single spatial dimension to produce a tensor of outputs\",\"It creates a convolution kernel that is convolved with the output layer over a multiple spatial dimension to produce a tensor of outputs\",\"It creates a convolution kernel that is convolved with the output layer over a single spatial dimension to produce a tensor of outputs\",\"It creates a convolution kernel that is convolved with the input layer over a multiple spatial dimension to produce a tensor of outputs\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Which of the following statements is False?\n",
        "Answer2 = \"\" #@param [\"\",\"Number of parameters in CNNs are usually less than the number of parameters in Feed forward Neural Networks\", \"CNNs are prone to overfitting because of less number of parameters\", \"There are no learnable parameters in Pooling layers\", \"Pooling layer operates on each feature map independently\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}