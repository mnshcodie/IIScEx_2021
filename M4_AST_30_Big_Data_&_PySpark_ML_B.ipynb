{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "M4_AST_30_Big_Data_&_PySpark_ML_B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnshcodie/IIScEx_2021/blob/main/M4_AST_30_Big_Data_%26_PySpark_ML_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gco6DBlYANg"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 30: Big Data, Cassandra and PySpark ML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3WtQbQ6YANo"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e7GNBKzYANp"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand the concept of NoSQL databases\n",
        "* understand the type of NoSQL databases available, i.e. Cassandra\n",
        "* implement Machine Learning using PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Ww4neyYANq"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-QUDI-wYANq"
      },
      "source": [
        "NoSQL ('Not only SQL') is a non-relational or distributed database system. It has a dynamic schema, best suited for hierarchical data storage. It is horizontally scalable.\n",
        "\n",
        "Whereas **SQL databases** are used for querying and manipulating structured query language (SQL) and are very powerful and versatile, they also have the disadvantage of being restrictive: SQL requires the usage of predefined schemas to determine the structure of the data before you work with it; Also all of your data must follow the same structure. This can require significant prior preparation, which offers challenges.\n",
        "\n",
        "A **NoSQL database** has dynamic schema for unstructured data. It stores data in many ways, such as document-oriented, column-oriented, graph-based or organized as a KeyValue. This flexibility means that documents can be created without having a prior defined structure. Also, each document can have its own unique structure. The syntax varies from database to database, and you can add fields as you go.\n",
        "\n",
        "Here are some key comparisons between SQL and NoSQL\n",
        "\n",
        "- **Scalability**: SQL databases are vertically scalable. This allows an increase in load on a single server by increasing the RAM, CPU, or SSD; NoSQL databases are horizontally scalable i.e. it can handle more traffic by sharding, or adding more servers in your NoSQL database\n",
        "- **Structure**: SQL databases are table-based. NoSQL databases are either key-value pairs, document-based, graph databases, or wide-column stores. Relational SQL databases are a better option for applications that require multi-row transactions such as an accounting system.\n",
        "- **Property**: SQL databases follow ACID properties (Atomicity, Consistency, Isolation and Durability) whereas the NoSQL databases follow the Brewers CAP theorem (Consistency, Availability and Partition tolerance). \n",
        "- **Examples**: Examples of SQL databases include PostgreSQL, MySQL, Oracle and Microsoft SQL Server. NoSQL database examples include Cassandra, MongoDB, BigTable, HBase, Neo4j and CouchDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTDlxz-LYANr"
      },
      "source": [
        "**Why we need NoSQL databases?**\n",
        "\n",
        "We see two primary reasons why people consider using a NoSQL database.\n",
        "* Productivity in Application development: A lot of application development effort is spent on mapping data between in-memory data structures and a relational database. A NoSQL database may provide a data model that better fits the application’s needs, thus simplifying that interaction and resulting in less code to write, debug, and evolve.\n",
        "* Large-scale data: Organizations are finding it valuable to capture more data and process it more quickly. They are finding it expensive, if even possible, to do so with relational databases. The primary reason is that a relational database is designed to run on a single machine, but it is usually more economic to run large data and computing loads on clusters of many smaller and cheaper machines. Many NoSQL databases are designed explicitly to run on clusters, so they make a better fit for big data scenarios.\n",
        "\n",
        "**Terminology**\n",
        "\n",
        "The basic terms related to NoSQL databases are as follows:\n",
        "\n",
        "* **Big data:** a collection of data that is huge in volume, yet growing exponentially with time.\n",
        "* **Polyglot persistent:** a term that refers to using different data stores in different circumstances.\n",
        "* **Database cluster:** a collection of databases that is managed by a single instance of a running database server. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sTpl9bnYANs"
      },
      "source": [
        "### Types of NoSQL Databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIjSDH-YANs"
      },
      "source": [
        "The following are the different types of NoSQL databases:\n",
        "\n",
        "* **Document databases** pair each key with a complex data structure known as a document. A document is a set of key-value pairs. MongoDB is an example of a document store database. A group of MongoDB documents is known as a collection. This is the equivalent of an RDBMS table.\n",
        "\n",
        "* **Graph stores** are used to store information about networks of data, for instance, social connections. Graph stores include Neo4J and Giraph.\n",
        "\n",
        "* **Key-value stores** databases store every single item in the database as a key together with its value. Examples of key-value stores are Riak and Berkeley DB. Some key-value stores, such as Redis, allow each value to have a type, such as an integer, which adds functionality.\n",
        "\n",
        "* **Wide-column** stores such as Cassandra and HBase are optimized for queries over large datasets, and store columns of data together, instead of rows.\n",
        "\n",
        "<figure>\n",
        "<img src='https://cdn.iisc.talentsprint.com/CDS/Images/Nosql_databases.png' />\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_30_Big_Data_&_PySpark_ML_B\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\") \n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/cal_housing.data\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/students.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/MiniProjects/secure-connect-cds.zip\")\n",
        "\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxCv5teXYANs"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB8vMNkUYANt"
      },
      "source": [
        "import pandas as pd\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZIn8pabYANt"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFILodbNYANu"
      },
      "source": [
        "students = pd.read_csv('students.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSlOKrW6YANu"
      },
      "source": [
        "### Cassandra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlBCo3h3YANu"
      },
      "source": [
        "Apache Cassandra is a free, open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers. It provides high availability with no single point of failure. It is a NoSQL database developed by Facebook. It is a great database that allows you to effectively run queries on a large amount of structured and semi-structured data.\n",
        "\n",
        "Cassandra has three containers, one within another. The outermost container is Keyspace. You can think of Keyspace as a database in the RDBMS land. Next, you will see the column family, which is like a table. Within a column family are columns, and columns are placed under rows. Each row is identified by a unique row key, similar to the primary key in RDBMS.\n",
        "\n",
        "<figure>\n",
        "<img src='https://cdn.iisc.talentsprint.com/CDS/Datasets/cassandra-data-model.ppm' />\n",
        "</figure>\n",
        "\n",
        "The difference from RDBMS is in the way Cassandra treats the data. Column families, unlike tables, can be schema free (schema optional). This means we can have different column names for different rows within the same column family. We can store about two billion columns per row. This means it can be very handy to store time-series data, such as tweets or comments on a blog post.\n",
        "\n",
        "Other than Cassandra, HBase is also a Wide-column store. For similarities and dissimilarities between them, refer [here](https://www.scnsoft.com/blog/cassandra-vs-hbase)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwDE6EgGYANv"
      },
      "source": [
        "#### Components of Cassandra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwVUoFTJYANv"
      },
      "source": [
        "Cassandra consists of the following components:\n",
        "\n",
        "<figure>\n",
        "<img src='https://cdn.iisc.talentsprint.com/CDS/Images/cassandra_cluster.jpg' width= 600 px/>\n",
        "</figure>\n",
        "\n",
        "**Node** is the place where data is stored. It is the basic component of Cassandra.\n",
        "\n",
        "**Data Center** A collection of nodes is called a data center. Many nodes are categorized as a data center.\n",
        "\n",
        "**Cluster** The cluster is the collection of many data centers.\n",
        "\n",
        "**Commit Log** Every write operation is written to Commit Log. Commit log is used for crash recovery.\n",
        "\n",
        "**Mem-table** After data written in Commit log, data is written in Mem-table. Data is written in Mem-table temporarily.\n",
        "\n",
        "**SSTable** When Mem-table reaches a certain threshold, data is flushed to an SSTable disk file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfsQvzLDYANw"
      },
      "source": [
        "#### Data Replication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLIYKlFGYANw"
      },
      "source": [
        "As hardware problems can occur or link can be down at any time during the data process, a solution is required to provide a backup when the problem has occurred. So data is replicated for assuring no single point of failure.\n",
        "\n",
        "Cassandra places replicas of data on different nodes based on these two factors.\n",
        "* Where to place the next replica is determined by the Replication Strategy.\n",
        "* While the total number of replicas placed on different nodes is determined by the Replication Factor.\n",
        "\n",
        "One Replication factor means that there is only a single copy of data while three replication factor means that there are three copies of the data on three different nodes.\n",
        "\n",
        "To know more about data replication click [here](https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/architecture/archDataDistributeReplication.html#:~:text=and%20fault%20tolerance.-,A%20replication%20strategy%2A)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K1zJStSYANw"
      },
      "source": [
        "#### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0sCkI1ZYANx"
      },
      "source": [
        "!pip install cassandra-driver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcSQWIpjYANy"
      },
      "source": [
        "import cassandra\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFV5X_4-YANy"
      },
      "source": [
        "print(cassandra.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87xlk-ydYANy"
      },
      "source": [
        "#### Connecting the database"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataStax**\n",
        "\n",
        "DataStax, Inc. is a data management company based in Santa Clara, California.\n",
        "Its product provides commercial support, software, and cloud database-as-a-service based on open source NoSQL database Apache Cassandra.\n",
        "\n",
        "We will be using its free tier version here."
      ],
      "metadata": {
        "id": "CXty6zbv93_5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XBDe2cxYANz"
      },
      "source": [
        "**Important:** Datastax account and keyspace creation steps provided below are encouraged but not mandatory. It will allow you to create your own cluster, perform data insertion and code execution steps end-to-end using your own credentials. Note that we have already inserted the data and provided the cluster connection through the CDS account in Datastax for the purpose of running this notebook.\n",
        "\n",
        "**For detailed instructions on account creation and keyspace creation**, please refer to this [document](https://cdn.iisc.talentsprint.com/CDS/DB_Connect_Docs/Instruction_for_Astra_Datastax_Database_Creation.pdf)\n",
        "\n",
        "**Astra Datastax login:** Login to [Datastax](https://www.datastax.com/) and create a database\n",
        "\n",
        "\n",
        "**Connect the database and create keyspace:**\n",
        "\n",
        "* Download Secure Connect Bundle zip file from Datastax [connect](https://docs.datastax.com/en/astra/docs/obtaining-database-credentials.html) section. Follow the instructions on the page\n",
        "* Upload the `Secure-connect-XXXX.zip` file, which is downloaded from Datastax.\n",
        "* Generate the token and save the credentials (.csv) from settings section.\n",
        "    * Hint: Select role as admin-user and generate token\n",
        "* Using the credentials generated in settings, specify the `client Id` and `Client Secret` to the variables below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5E-RZFDYAN0"
      },
      "source": [
        "Set the `Secure connect bundle zip file` path and specify the `clientID` and `Client_Secret`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSKSWS6YAN0"
      },
      "source": [
        "zip_path = '/content/secure-connect-cds.zip'\n",
        "Client_ID = 'SzdMZDsXLvXUQiHRsEogQgtR'\n",
        "Client_Secret = 'SaYcoWaejFAx4CxXzuf1spOMRa+t1oyTd8Z+Medbuba1q0Ww5AY,1MOPNvrr9wWSnR82,IiQa4muFoF8OfOhxdndNXtZbuZsv.dSwKKccUaHr96B8-88gyAWGURFO2Wa'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAl8fCWbYAN0"
      },
      "source": [
        "#### Create a Cluster instance to connect to your Astra database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwLOKebeYAN0"
      },
      "source": [
        "You will typically have one instance of Cluster for each Cassandra cluster you want to interact with. Create a session object using the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfZjj9-VYAN1"
      },
      "source": [
        "cloud_config= {\n",
        "        'secure_connect_bundle': zip_path\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(Client_ID,  Client_Secret)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcWJMBbHYAN1"
      },
      "source": [
        "session = cluster.connect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOnLHG86YAN1"
      },
      "source": [
        "#### Verifying the database connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Ok8uG1YAN1"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLNCMbEcYAN2"
      },
      "source": [
        "#### Setting the Key Space in database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOiE5IkbYAN2"
      },
      "source": [
        "A keyspace is the top-level database object that controls the replication for the object it contains at each datacenter in the cluster. Keyspaces contain tables, materialized views and user-defined types, functions and aggregates. Typically, a cluster has one keyspace per application. Since replication is controlled on a per-keyspace basis, store data with different replication requirements (at the same datacenter) in different keyspaces.\n",
        "\n",
        "Before creating tables and inserting data let us create and set the keyspace\n",
        "* we can either create keyspace manually on Datastax dashboard or using the CQL command. \n",
        "[Hint](https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlCreateKeyspace.html)\n",
        "* once the keyspace is created successfully, set the keyspace using the command `set_keyspace()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf5soxJGYAN2"
      },
      "source": [
        "try:\n",
        "    session.set_keyspace('ast_student')\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVgx_nKBYAN2"
      },
      "source": [
        "#### Insert the data into Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpRmkEa_YAN2"
      },
      "source": [
        "# Display few rows of students dataframe\n",
        "students.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDvFopt5YAN3"
      },
      "source": [
        "For the following data insertion step we have already uploaded the data on Datastax using the CDS account, so you are not required to insert the data again. Therefore we have commented out the below lines of code.\n",
        "\n",
        "However, if you would like to perform the data insertion step then please **create your own account** on Datastax as given in the reference [here](https://cdn.iisc.talentsprint.com/CDS/DB_Connect_Docs/Assignment_Datastax_Connect.pdf) and change the credentials and run the below code by uncommenting it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyr6eyfuYAN3"
      },
      "source": [
        "**Create a column family in keyspace and insert the data using CQL command**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc_m8cxWYAN3"
      },
      "source": [
        "# Creating the students table\n",
        "# query = \"\"\"CREATE TABLE  IF NOT EXISTS students (studentID INT,\n",
        "#                                                 name TEXT,\n",
        "#                                                 age INT,\n",
        "#                                                 marks INT,\n",
        "#                                                 PRIMARY KEY (studentID)); \"\"\"\n",
        "# try:\n",
        "#     session.execute(query)\n",
        "# except Exception as e:\n",
        "#     print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byOXMr1PYAN4"
      },
      "source": [
        "#students_cols = ','.join(students.columns.values)\n",
        "#for (i,row) in students.iterrows():\n",
        "#    query = 'INSERT INTO ast_student.students ({}) VALUES (%s, %s, %s, %s)'.format(students_cols)\n",
        "#    session.execute(query, tuple(row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meOLB8v6YAN4"
      },
      "source": [
        "#### Querying the database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJn7_lK9YAN4"
      },
      "source": [
        "Select first 5 rows of the students table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RqNAgCAYAN4"
      },
      "source": [
        "query = 'SELECT * FROM ast_student.students LIMIT 5;'\n",
        "rows=session.execute(query)\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-36JlnWYAN5"
      },
      "source": [
        "Select the count of records where marks are above 85"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W2yhr3sYAN5"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX4MsumrYAN5"
      },
      "source": [
        "#### Updating the database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRl4x3yUYAN5"
      },
      "source": [
        "**Uncomment and run the below line of code only if you are using your own credentials , to not affect original database given from DLFA account.**\n",
        "\n",
        "Update the value of the marks to 98 in the document where studendID is 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crxzrfyhYAN6"
      },
      "source": [
        "# Query = 'UPDATE ast_student.students SET marks = 98 WHERE studentID = 2;'\n",
        "# session.execute(Query)\n",
        "\n",
        "# query = 'SELECT * FROM ast_student.students LIMIT 5;'\n",
        "# rows = session.execute(query)\n",
        "# for row in rows:\n",
        "#     print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhPAQB3AYAN6"
      },
      "source": [
        "To verify the tables in the keyspace below CQL command will be helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dptTsUC9YAN6"
      },
      "source": [
        "query = \"SELECT * FROM system_schema.tables WHERE keyspace_name = 'ast_student';\"\n",
        "rows = session.execute(query)\n",
        "for row in rows:\n",
        "    print(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsIaxiIMYAN6"
      },
      "source": [
        "#### Drop table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNH9oy-aYAN6"
      },
      "source": [
        "It is not advisable to delete a table but to reduce space we might sometimes need to delete tables from the Datastax database.\n",
        "\n",
        "**Uncomment and run the below line of code only if you are using your own credentials , to not affect original database given from DLFA account.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxnYsYTnYAN7"
      },
      "source": [
        "# query = \"DROP TABLE IF EXISTS ast_student.students;\"\n",
        "# session.execute(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOEubnoQYAN7"
      },
      "source": [
        "#### Close the session and cluster connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hhyn-SaYAN7"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning using PySpark\n",
        "Predicting House Prices using California Housing Dataset"
      ],
      "metadata": {
        "id": "jLyDqEafOmYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll make use of the California Housing data set. Note, of course, that this is actually 'small', but, the purpose of this notebook is meant to give you an idea of how we can use PySpark to build a machine learning model."
      ],
      "metadata": {
        "id": "3zbRzn_2WIoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description** : The California Housing data set appeared in a 1997 paper titled Sparse Spatial Autoregressions, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n",
        "\n",
        "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
        "\n",
        "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
        "\n",
        "`Longitude`:refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n",
        "\n",
        "`Latitude` :refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n",
        "\n",
        "`Housing Median Age`:is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n",
        "\n",
        "`Total Rooms`:is the total number of rooms in the houses per block group\n",
        "\n",
        "`Total Bedrooms`:is the total number of bedrooms in the houses per block group\n",
        "\n",
        "`Population`:is the number of inhabitants of a block group\n",
        "\n",
        "`Households`:refers to units of houses and their occupants per block group\n",
        "\n",
        "`Median Income`:is used to register the median income of people that belong to a block group\n",
        "\n",
        "`Median House Value`:is the dependent variable and refers to the median house value per block group\n",
        "\n",
        "\n",
        "The Median house value is the dependent variable and will be assigned the role of the target variable in our ML model."
      ],
      "metadata": {
        "id": "5nZn6MKYWgiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the required libraries and packages"
      ],
      "metadata": {
        "id": "tqvn3NfldIEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "O68_-GPiXoG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf, col\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BNbIkL9DXtKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_colwidth', 400)\n",
        "\n",
        "from matplotlib import rcParams\n",
        "sns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\n",
        "rcParams['figure.figsize'] = 18,4\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "LTce0dhxX55K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting random seed\n",
        "rnd_seed=23\n",
        "np.random.seed=rnd_seed\n",
        "np.random.set_state=rnd_seed"
      ],
      "metadata": {
        "id": "RS3bziWhX67h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the Spark Session"
      ],
      "metadata": {
        "id": "jegkXn3xX_hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WubepuVEX9EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Spark Context"
      ],
      "metadata": {
        "id": "jcR__JpMSJ6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "id": "7Kv9LhAFYF2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating SQL Context"
      ],
      "metadata": {
        "id": "lDLlDzRpSNfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqlContext = SQLContext(spark.sparkContext)\n",
        "sqlContext"
      ],
      "metadata": {
        "id": "0ppSUWhNYK7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load The Data From the File"
      ],
      "metadata": {
        "id": "b57grHvvYPLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HOUSING_DATA = '/content/cal_housing.data'"
      ],
      "metadata": {
        "id": "Kaun7WFJYOy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the schema when loading data into a DataFrame will give better performance than schema inference."
      ],
      "metadata": {
        "id": "jSrwug1lXq5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema, corresponding to a line in the csv data file.\n",
        "schema = StructType([\n",
        "    StructField(\"long\", FloatType(), nullable=True),\n",
        "    StructField(\"lat\", FloatType(), nullable=True),\n",
        "    StructField(\"medage\", FloatType(), nullable=True),\n",
        "    StructField(\"totrooms\", FloatType(), nullable=True),\n",
        "    StructField(\"totbdrms\", FloatType(), nullable=True),\n",
        "    StructField(\"pop\", FloatType(), nullable=True),\n",
        "    StructField(\"houshlds\", FloatType(), nullable=True),\n",
        "    StructField(\"medinc\", FloatType(), nullable=True),\n",
        "    StructField(\"medhv\", FloatType(), nullable=True)]\n",
        ")"
      ],
      "metadata": {
        "id": "aLQDkA4-YYll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load housing data\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "LRqtjRYuYbod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect first five rows\n",
        "housing_df.take(5)"
      ],
      "metadata": {
        "id": "5pqBm42nYd4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first five rows\n",
        "housing_df.show(5)"
      ],
      "metadata": {
        "id": "zmXg02AnYgNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the dataframe columns\n",
        "housing_df.columns"
      ],
      "metadata": {
        "id": "-OpkyOitYjd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the schema of the dataframe\n",
        "housing_df.printSchema()"
      ],
      "metadata": {
        "id": "zLfr1YhyYmBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration"
      ],
      "metadata": {
        "id": "CwYmcgSKXF_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a sample selection\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "qrQ7_tk3YxTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distribution of the median age of the people living in the area"
      ],
      "metadata": {
        "id": "khdWEHL8ZCOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by housing median age and see the distribution\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "iQRixh37ZIPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.show(10)"
      ],
      "metadata": {
        "id": "NuUu_qf6ZKWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.toPandas().plot.bar(x='medage',figsize=(14, 6))"
      ],
      "metadata": {
        "id": "F4dqFsBbZNLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the residents are either in their youth or they settle here during their senior years. Some data are showing median age < 10 which seems to be out of place."
      ],
      "metadata": {
        "id": "7kWbLAAeZQlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary Statistics\n",
        "Spark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame."
      ],
      "metadata": {
        "id": "YRZxvrYXZazY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(housing_df.describe().select(\n",
        "                    \"summary\",\n",
        "                    F.round(\"medage\", 4).alias(\"medage\"),\n",
        "                    F.round(\"totrooms\", 4).alias(\"totrooms\"),\n",
        "                    F.round(\"totbdrms\", 4).alias(\"totbdrms\"),\n",
        "                    F.round(\"pop\", 4).alias(\"pop\"),\n",
        "                    F.round(\"houshlds\", 4).alias(\"houshlds\"),\n",
        "                    F.round(\"medinc\", 4).alias(\"medinc\"),\n",
        "                    F.round(\"medhv\", 4).alias(\"medhv\"))\n",
        "                    .show())"
      ],
      "metadata": {
        "id": "yvtyaiIKZlL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize the dataset."
      ],
      "metadata": {
        "id": "Orq7db7MZn_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "With all this information that we gathered from our exploratory data analysis, we know enough to preprocess our data to feed it to the model.\n",
        "\n",
        "\n",
        "* We should probably standardize our data, as we have seen that the range of minimum and maximum values is quite big.\n",
        "\n",
        "* There are possibly some additional attributes that we could add, such as a feature that registers the number of bedrooms per room or the rooms per household.\n",
        "\n",
        "* Our dependent variable is also large in value; To make it easier to work with it, we'll have to slightly adjust the values."
      ],
      "metadata": {
        "id": "0G3QsvbZZqj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing The Target Values\n",
        "\n",
        "First, let's start with the medianHouseValue, our dependent variable. To facilitate our working with the target values, we will express the house values in units of 100,000. That means that a target such as 452600.000000 should become 4.526."
      ],
      "metadata": {
        "id": "mGclV3HdaP7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the values of `medianHouseValue`\n",
        "housing_df = housing_df.withColumn(\"medhv\", col(\"medhv\")/100000)"
      ],
      "metadata": {
        "id": "om6Ht0rwae1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 2 lines of `df`\n",
        "housing_df.show(2)"
      ],
      "metadata": {
        "id": "51BUNk1zahZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that the values have been adjusted correctly when we look at the result of the show() method."
      ],
      "metadata": {
        "id": "WHOGyeksaj9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "VHitBip4anG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have adjusted the values in medianHouseValue, we will now add the following columns to the data set:\n",
        "\n",
        "\n",
        "\n",
        "*   Rooms per household which refers to the number of rooms in households per block group;\n",
        "\n",
        "*   Population per household, which basically gives us an indication of how many people live in households per block group; And\n",
        "*   Bedrooms per room which will give us an idea about how many rooms are bedrooms per block group;\n",
        "\n",
        "As we're working with DataFrames, we can best use the select() method to select the columns that we're going to be working with, namely totalRooms, households, and population. Additionally, we have to indicate that we're working with columns by adding the col() function to our code. Otherwise, we won't be able to do element-wise operations like the division that we have in mind for these three variables.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZgBUAcBaoV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_df.columns"
      ],
      "metadata": {
        "id": "wmjW3cl3c3sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the new columns to `df`\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "FJiaqvwZc6ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the result\n",
        "housing_df.show(5)"
      ],
      "metadata": {
        "id": "6h0C8KJyc9RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that, for the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.5 people and the amount of bedrooms is quite low with 0.14.\n",
        "\n",
        "Since we don't want to necessarily standardize our target values, we'll want to make sure to isolate those in our data set. Note also that this is the time to leave out variables that we might not want to consider in our analysis. In this case, let's leave out variables such as longitude, latitude, housingMedianAge and totalRooms.\n",
        "\n",
        "In this case, we will use the select() method and passing the column names in the order that is more appropriate. In this case, the target variable medianHouseValue is put first, so that it won't be affected by the standardization."
      ],
      "metadata": {
        "id": "jd9lF6XqdCPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-order and select columns\n",
        "housing_df = housing_df.select(\"medhv\", \n",
        "                              \"totbdrms\", \n",
        "                              \"pop\", \n",
        "                              \"houshlds\", \n",
        "                              \"medinc\", \n",
        "                              \"rms_per_hh\", \n",
        "                              \"pop_per_hh\", \n",
        "                              \"bdrms_per_rm\")"
      ],
      "metadata": {
        "id": "nE117HG0dVIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Extraction\n",
        "\n",
        "Now that we have re-ordered the data, we're ready to normalize the data. We will choose the features to be normalized."
      ],
      "metadata": {
        "id": "YuWiZwoedX-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featureCols = [\"totbdrms\", \"pop\", \"houshlds\", \"medinc\", \"rms_per_hh\", \"pop_per_hh\", \"bdrms_per_rm\"]"
      ],
      "metadata": {
        "id": "W0Br4YTOdd9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use a VectorAssembler to put features into a feature vector column**"
      ],
      "metadata": {
        "id": "NgMFGtcKdgRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put features into a feature vector column\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "UZxE_YVedmF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembled_df = assembler.transform(housing_df)"
      ],
      "metadata": {
        "id": "D8jhDdLQdoA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembled_df.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "tWQ5nZGsdqIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the features have transformed into a Dense Vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "zamnqcAmdx6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Standardization\n",
        "\n",
        "Next, we can finally scale the data using StandardScaler. The input columns are the features, and the output column with the rescaled values that will be included in the scaled_df will be named \"features_scaled\"."
      ],
      "metadata": {
        "id": "lOd7w4eSd0ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the `standardScaler`\n",
        "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")"
      ],
      "metadata": {
        "id": "l5G28Ivod8z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the DataFrame to the scaler\n",
        "scaled_df = standardScaler.fit(assembled_df).transform(assembled_df)"
      ],
      "metadata": {
        "id": "tCCiEOhBd9jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the result\n",
        "scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)"
      ],
      "metadata": {
        "id": "VggKmWUad_3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building A Machine Learning Model With Spark ML"
      ],
      "metadata": {
        "id": "BxrW-IOseFNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all the preprocessing done, it's finally time to start building our Linear Regression model! Just like always, we first need to split the data into training and test sets. Luckily, this is no issue with the randomSplit() method:"
      ],
      "metadata": {
        "id": "YNAYeRqaeKUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WP3iLRxfeMQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass in a list with two numbers that represent the size that we want training and test sets to have including a seed.\n",
        "\n",
        "Note that the argument elasticNetParam corresponds to  α  or the vertical intercept and that the regParam or the regularization paramater corresponds to  λ ."
      ],
      "metadata": {
        "id": "hUKlWdmweO91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "cNWQusu8eRti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create an ElasticNet model**\n",
        "\n",
        "ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both."
      ],
      "metadata": {
        "id": "5WR8wFaNeUYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize `lr`\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "uiQP6iase8yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the data to the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "eWBVOqNee_JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Model\n",
        "\n",
        "With our model in place, we can generate predictions for our test data: use the transform() method to predict the labels for our test_data. Then, we can use RDD operations to extract the predictions as well as the true labels from the DataFrame."
      ],
      "metadata": {
        "id": "IU8ldByRfCRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspect the Model Co-efficients"
      ],
      "metadata": {
        "id": "w4tbh_wLfVPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficients for the model\n",
        "linearModel.coefficients"
      ],
      "metadata": {
        "id": "MGun7VXjfZXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureCols"
      ],
      "metadata": {
        "id": "pG0J_Q4nfcev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intercept for the model\n",
        "linearModel.intercept"
      ],
      "metadata": {
        "id": "A_gAhFS3ffkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\n",
        "coeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]"
      ],
      "metadata": {
        "id": "Vlc1rXtlfiDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coeff_df"
      ],
      "metadata": {
        "id": "wX_uGUMUfj-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Predictions"
      ],
      "metadata": {
        "id": "8FzalDN5fY-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "5QNIiz4jfqSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the predictions and the \"known\" correct labels\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "VAGJ0SyxfsXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspect the Metrics\n",
        "\n",
        "Looking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good our model actually is.\n",
        "\n",
        "Here, we will use the LinearRegressionModel.summary attribute.\n",
        "\n",
        "Next, we can also use the summary attribute to pull up the rootMeanSquaredError and the r2."
      ],
      "metadata": {
        "id": "Ht6AMM-QfxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the RMSE\n",
        "print(\"RMSE: {0}\".format(linearModel.summary.rootMeanSquaredError))"
      ],
      "metadata": {
        "id": "urKx_AU6gCD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE: {0}\".format(linearModel.summary.meanAbsoluteError))"
      ],
      "metadata": {
        "id": "meQFEK4dgEco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the R2\n",
        "print(\"R2: {0}\".format(linearModel.summary.r2))"
      ],
      "metadata": {
        "id": "-fpE6kzIgGzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the RegressionEvaluator from pyspark.ml package**"
      ],
      "metadata": {
        "id": "mnkCADy9gSZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the RMSE\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "52v3uAhagYTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the MAE\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "I4nHYuvLgbTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the R2\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "F4Zc3OFRgdSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the RegressionMetrics from pyspark.mllib package**\n",
        "\n"
      ],
      "metadata": {
        "id": "hw1Ozhfzgf09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mllib is old that is why the methods are available in rdd\n",
        "metrics = RegressionMetrics(predandlabels.rdd)"
      ],
      "metadata": {
        "id": "I_uz0MwLgj8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE: {0}\".format(metrics.rootMeanSquaredError))"
      ],
      "metadata": {
        "id": "8TZiMzu1gmk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE: {0}\".format(metrics.meanAbsoluteError))"
      ],
      "metadata": {
        "id": "tX-28sEjgoo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R2: {0}\".format(metrics.r2))"
      ],
      "metadata": {
        "id": "cQyOYD6ggq7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's definitely some improvements needed to our model! If we want to continue with this model, we can play around with the parameters that we passed to the model, the variables that we included in the original DataFrame."
      ],
      "metadata": {
        "id": "L3WLlyJjgqo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "Tooxdg2CgxrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a NoSQL Database?\n",
        "\n",
        "A. NoSQL is also referred as Not only SQL to emphasize that they may support SQL-like query language used in relational database.\n",
        "\n",
        "B. NoSQL database provides a mechanism to store and retrieve data, which are modeled rather than the tabular relations used in Relational databases.\n",
        "\n",
        "C. There are majorly 4 types of NoSQL Databases,\n",
        "\n",
        "\n",
        "*   Key Value Store\n",
        "*   Document Store\n",
        "*   Column Store\n",
        "*   Graph Databases"
      ],
      "metadata": {
        "id": "mBWxH948FGGV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHDYadgG-RHl",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which is TRUE for a NoSQL?\n",
        "Answer1 = \"\" #@param [\"\",\"Only A and B\",\"Only B and C\",\"Only A nand C\", \"A, B, C\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the statements given below:\n",
        "\n",
        "P: The system allows operations all the time, and operations return quickly\n",
        "\n",
        "Q: All nodes see same data at any time, or read operations return latest written value by any client\n",
        "\n",
        "R: The system continues to work in spite of network partitions"
      ],
      "metadata": {
        "id": "qj9C17QoNw45"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu3sufDD7pu1",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Identify the correct choices for the above given statements:\n",
        "Answer2 = \"\" #@param [\"\",\"P: Consistency, Q: Availability, R: Partition tolerance\",\"P: Availability, Q: Consistency, R: Partition tolerance\",\"P: Partition tolerance, Q: Consistency, R: Availability\",\"P: Consistency, Q: Partition tolerance, R: Availability\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}