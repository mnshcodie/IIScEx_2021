{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2_AST_13_PCA_&_SVM_Regression_B.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnshcodie/IIScEx_2021/blob/main/M2_AST_13_PCA_%26_SVM_Regression_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnXB0QQcmFEv"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 13: Principal Component Analysis (PCA) and Support Vector Machines - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1sCMGb_m117"
      },
      "source": [
        "## Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDEDgaW1myYD"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* compute co-variance matrix\n",
        "* calculate the eigen values and eigen vectors\n",
        "* apply PCA using sklearn package\n",
        "* Perform PCA using selected no of principal components\n",
        "* apply support vector classifier on the PCA reduced data\n",
        "* perform support vector machines - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTZeUO5Bfi3"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQKsWtE5BcIu"
      },
      "source": [
        "### History\n",
        "\n",
        "Breast cancer (BC) is one of the most common cancers among women in the world today. Currently, the average risk of a woman in the United States developing breast cancer sometime in her life is about 13%, which means there is a 1 in 8 chance she will develop breast cancer!. An early diagnosis of BC can greatly improve the prognosis and chance of survival for patients. Thus an accurate identification of malignant tumors is of paramount importance.\n",
        "\n",
        "### Description\n",
        "\n",
        "The Breast cancer Data Set consists of 569 data instances. This is a binary classification problem which consists of 2 types of cancer classes. The tumor is classified as **benign (1)** or **malignant (0)** based on its geometry and shape. \n",
        "\n",
        "The features of the dataset include:\n",
        "\n",
        "* ID number\n",
        "* Diagnosis (M = malignant, B = benign) \n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "* radius (mean of distances from center to points on the perimeter) \n",
        "* texture (standard deviation of gray-scale values) \n",
        "* perimeter \n",
        "* area \n",
        "* smoothness (local variation in radius lengths) \n",
        "* compactness (perimeter^2 / area - 1.0) \n",
        "* concavity (severity of concave portions of the contour) \n",
        "* concave points (number of concave portions of the contour) \n",
        "* symmetry \n",
        "* fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "\n",
        "The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 32 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "Class distribution: 357 benign, 212 malignant\n",
        "\n",
        "Data source to this experiment : https://archive.ics.uci.edu/ml/datasets/breast+cancer\n",
        "\n",
        "This analysis aims to observe which features are most helpful in predicting malignant or benign cancer. The goal is to classify whether the breast cancer is benign or malignant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljuhF_JxCq8E"
      },
      "source": [
        "## Domain Information\n",
        "\n",
        "Many machine learning problems involve a vast number of features for each training instance, making the training extremely slow and harder to find a good solution. This problem is called the **curse of dimensionality**. Therefore, we need dimensionality reduction techniques, that transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains most meaningful properties of the original data. Dimensionality reduction speeds up training and is also extremely useful for data visualization. One of the main approaches to reducing dimensionality is data projection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaeFgzsNDZgA"
      },
      "source": [
        "#### Projection\n",
        "\n",
        "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances lie within a much lower-dimensional subspace of the high-dimensional space.\n",
        "\n",
        "In the figure below a 3D dataset is represented by circles.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/pca1.png\" width=450px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{8.6cm} \\text{A 3D dataset lying close to a 2D subspace}$\n",
        "<br><br>\n",
        "\n",
        "Notice that all training instances lie close to a plane which is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. \n",
        "\n",
        "After projecting every training instance perpendicularly onto this subspace, we get the new 2D dataset as shown in the figure below. By that, we reduced the dataset’s dimensionality from 3D to 2D.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"data:image/png;base64,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\" width=350px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{10cm} \\text{The new 2D dataset after projection}$\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_xwYDHlEAQL"
      },
      "source": [
        "The most common dimensionality reduction method, that applies projection is:\n",
        "\n",
        "* PCA (Principal Component Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exG368oL8jv2",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M2_AST_13_PCA_&_SVM_Regression_B\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/data.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Housing_data.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVrZPM_ICJoy"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BodsbCWT1VlC"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-zYvHDCCM-X"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JRBbiUJ0mnV"
      },
      "source": [
        "breast_cancer = pd.read_csv('data.csv')\n",
        "breast_cancer.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHM4ENjP4nR7"
      },
      "source": [
        "# The dataset has 569 instances and 32 features\n",
        "print(breast_cancer.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9e6tMpx7LHE"
      },
      "source": [
        "Get a count of the number of Malignant (M) or Benign (B) cells and visualize the count.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVBSIyK67eoC"
      },
      "source": [
        "breast_cancer['diagnosis'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvIQU_Hn7JN9"
      },
      "source": [
        "sns.countplot(breast_cancer['diagnosis'], label = 'count');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7NsWQtL6KLO"
      },
      "source": [
        "### Storing features and labels from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvjOow_w1-8S"
      },
      "source": [
        "# YOUR CODE HERE: Extracting features from cancer data and drop the 'diagnosis', 'id' column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZj1vtvm6khe"
      },
      "source": [
        "# YOUR CODE HERE: Extracting labels from cancer data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO3-jBm2719T"
      },
      "source": [
        "Encode the categorical data values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDmxpcAR70fr"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91N7O6ES8Pm3"
      },
      "source": [
        "encoded_labels[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbR62ndl20p"
      },
      "source": [
        "Let's check the correlation between the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdKFbEOSmI4e"
      },
      "source": [
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(features.corr(), annot=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRQOfR1SG-uK"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        " \n",
        "It is defined as the orthogonal transformation of the data into a series of uncorrelated principal components such that the first component explains the most variance in the data with each subsequent component explaining less.\n",
        "\n",
        "This technique is particularly useful in processing data where multi-colinearity exists between features or when the dimensions of features are high. Using PCA we project the data on a subset of input space.\n",
        "\n",
        "PCA is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of data on a subset of input space that still contains most of the information in the large set and we project data on the subspace with maximum variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp0z1u4Bpa_j"
      },
      "source": [
        "###  Mathematical calculation of eigen values and eigen vectors\n",
        "\n",
        "Step 1:   Standardization\n",
        "\n",
        "Step 2:   Compute the Covariance matrix\n",
        "\n",
        "Step 3:  Compute the Eigenvalues and Eigenvectors of the Covariance matrix to identify the principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKB-3dpn6-sm"
      },
      "source": [
        "#### 1. Standardization (Scaling the data)\n",
        "\n",
        "Standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. For instance all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function.\n",
        "\n",
        "So, for each observed value of the variable, subtract the mean and divide by the standard deviation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zd7IuQEZuQh"
      },
      "source": [
        "# Define a function for standardization of the data\n",
        "def standardization(features):\n",
        "\n",
        "    # Loop through each feature in the dataset\n",
        "    for feature_name in features:\n",
        "\n",
        "        # For each feature subtract the mean \n",
        "        features[feature_name] -= features[feature_name].mean()\n",
        "\n",
        "        # Divide by the standard deviation\n",
        "        features[feature_name] /= features[feature_name].std()\n",
        "        \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMye4vBdYnrE"
      },
      "source": [
        "# Call the 'standardization' function by passing the dataset features\n",
        "scaled_data = standardization(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Mjj6XtI8gz"
      },
      "source": [
        "# Print the first five rows of the scaled data\n",
        "scaled_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NalQljb5e6s"
      },
      "source": [
        "#### 2. Compute the covariance matrix\n",
        "\n",
        "For a given dataset $X = {x_1, . . . , x_N }, X ∈ R^D $. Compute the matrix $\\boldsymbol X\\boldsymbol X^T$ (a $N$ by $N$ matrix with $N \\ll D$), where D is the dimension of the sample in the dataset, and N is the number of samples in the dataset.\n",
        "\n",
        "Covariance is to measure how the variables of the input dataset are varying from the mean with respect to each other. It gives the variance between each feature in our original dataset. \n",
        "\n",
        "The covariance matrix is a d × d symmetric matrix (d is the number of dimensions) where each element represents the covariance between two features.\n",
        "\n",
        "Assume $X$ is the standardized data, then Co-variance Matrix of $X$ would be, $Σ =\\tfrac{X^T.X}{n - 1} $ \n",
        "\n",
        "where $X^T$ = Transpose of matrix $X$ and n is number of samples. \n",
        "\n",
        "http://www.stat.ucdavis.edu/~xdgli/Xiaodong_Li_Teaching_files/135Note2.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKNVV0RsbKOK"
      },
      "source": [
        "# Number of samples\n",
        "n = scaled_data.shape[0]\n",
        "\n",
        "# YOUR CODE HERE: Compute the co-variance matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VJ-IA8j7tKJ"
      },
      "source": [
        "#### 3. Compute the Eigenvalues and Eigenvectors \n",
        "\n",
        "After the covariance matrix is generated, eigen decomposition is performed on the covariance matrix. Eigenvectors and Eigenvalues are found as a result of the eigen decomposition. \n",
        "\n",
        "Eigenvectors of a covariance matrix are actually the directions of the axes where there is a most variance (most information).\n",
        "\n",
        "Eigenvalues (importance of these different directions) are coefficients attached to each Eigenvector. Each Eigenvector has a corresponding Eigenvalue, and the sum of the Eigenvalues represents all of the variance within the entire dataset.\n",
        "\n",
        "Compute eigenvalues $\\lambda$s and eigenvectors $V$ for $\\boldsymbol X\\boldsymbol X^T$ matrix. Compute the eigenvectors for the original covariance matrix as $\\boldsymbol X^T\\boldsymbol V$. Choose the eigenvectors associated with the 'M (cov_mat)' largest eigenvalues to be the basis of the principal subspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r5gyUOKB1-2"
      },
      "source": [
        "# YOUR CODE HERE: To find eigenvalues and corresponding eigenvectors for cov_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hAqDlaDEkYg"
      },
      "source": [
        "Eigenvectors are the principal components. The first principal component is the first column and the second principal component is the second column and so on. Each Eigenvector will correspond to an Eigenvalue, each eigenvector can be scaled of its eigenvalue, whose magnitude indicates how much of the data’s variability is explained by its eigenvector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC0kYA6baV4W"
      },
      "source": [
        "Arrange the eigen values in the descending order and compute the variance explained for each dimension. It provides the percentage of variance explained at each of the dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJMjHl68VBIA"
      },
      "source": [
        "# Total sum of the eigen values\n",
        "tot = sum(eig_vals)\n",
        "# Percentage of variance for each principal component\n",
        "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
        "print(var_exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86cYpnh5a_oT"
      },
      "source": [
        "Cumulative sum provides the percentage of variance accounted for by the first n components.\n",
        "\n",
        "For example, the cumulative percentage for the second component is the sum of the percentage of variance for the first and second component and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgMwjZ2bVDxz"
      },
      "source": [
        "# YOUR CODE HERE: Find the cumulative sum for the variance explained by each dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfkl1YX0bKIG"
      },
      "source": [
        "plot the cumulative sum variance as a function of the number of components/dimensions and check for an elbow in the curve to select the principal components. Cumulative sum is used to display the total sum of data as it grows with each component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I8nOa4PZwoS"
      },
      "source": [
        "plt.plot(cumsum)\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('Captured variance') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHguQAKxNEJ1"
      },
      "source": [
        "Since eigen values capture the variance by each component in the direction of the eigen vector. We can see the percentage of variance contributed by each feature which in turn contributes to the predicting power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOEUp10u77jG"
      },
      "source": [
        "plot to find the maximum variance between each principal component\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0CS8n_D78Bt"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 8))\n",
        "# Plot the explained variance around each dimesnion\n",
        "plt.bar(range(1, len(var_exp)+1), var_exp)\n",
        "plt.xticks(np.arange(0, 35, 5))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Percentage of variance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHi3k7agqSeO"
      },
      "source": [
        "The first seven components together covering most of the information about (91%). So, the remaining components can be dropped without losing much information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nL14qyyQqEl"
      },
      "source": [
        "We can then compute the projection and reconstruction of the data onto the space spanned by the top $n$ eigenvectors (no. of principal components).\n",
        "\n",
        "We compute the projection matrix onto the space spanned by `B` where, B: ndarray of dimension (D, M), the basis for the subspace where D is the dimension of the data, and N is the number of datapoints which returns the projection matrix.\n",
        "\n",
        "we assume there exists a low-dimensional compressed representation $z_n = B^T X ∈ R^M$ of $X$, where we define the projection matrix $B = [b_1,...b_M] ∈ R^{D×M}$. We assume that the columns of B are orthonormal so that $b_i^T b_j = 0$ if and only if $i \\neq j$ and $b_i^T b_j = 1$. We seek an M-dimensional subspace $U ⊆ R^D$, $dim(U) = M < D$ onto which we project the data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta59ipNtMcjp"
      },
      "source": [
        "# Select the top 7 eigen vectors (principal components)\n",
        "B = eig_vecs[:, :7]\n",
        "\n",
        "# Dimensionality reduction of the original data to 7 principal compoenents\n",
        "X = scaled_data.dot(B)\n",
        "\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjCNfAc0NzOP"
      },
      "source": [
        "# Reconstruct the data from the lower dimensional representation\n",
        "X_reconstruct = X.dot(B.T)\n",
        "print(X_reconstruct.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XylLLs8jdD7w"
      },
      "source": [
        "### Applying PCA using sklearn package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu7mmMwM8gQG"
      },
      "source": [
        "Scikit-Learn’s `PCA` class uses singular value decomposition (SVD) decomposition to implement PCA.\n",
        "\n",
        "To know more about Scikit-Learn’s `PCA` class, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY07w5zenAMF"
      },
      "source": [
        "#### Apply PCA on the scaled data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyyTGzc78Zwr"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit_transform(scaled_data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtGkvX81viLD"
      },
      "source": [
        "#### Principal Components using the Explained Variance Ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3D98KhZAD7z"
      },
      "source": [
        "pca.explained_variance_ratio_ parameter returns a vector of the variance explained for each dimension.\n",
        "\n",
        "PCA function provides explained_variance_ratio_ which gives the percentage of explained variance at each of the selected components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpwmxdGa84HB"
      },
      "source": [
        "# YOUR CODE HERE: To compute the explained variance ratio and sort the variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdihG9kwITZH"
      },
      "source": [
        "44.27% of variance on the data is explained by the first principal component, the second principal component explains 18.97% of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT40ioCnAnri"
      },
      "source": [
        "Cumulative variance in PCA gives the percentage of variance accounted for by the first n components.\n",
        "\n",
        "For example, the cumulative percentage for the second component is the sum of the percentage of variance for the first and second component and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KQRoZ4VAHzL"
      },
      "source": [
        "cumsum_explained_variance = np.cumsum(variance)\n",
        "print(cumsum_explained_variance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDVhxrq0OI4-"
      },
      "source": [
        "If we use the first feature, it will explain 44.27% of the data; If we consider two features we can capture 63.2% of the data. If we use all features we can describe the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzNDgQFToG7D"
      },
      "source": [
        "#### Determining how many number of components\n",
        "\n",
        "Instead of arbitrarily choosing the number of dimensions to reduce down to, we can either\n",
        "\n",
        "* select 2 or 3 dimensions for data visualization\n",
        "\n",
        "* choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., `PCA(n_components=0.92)` for 92% variance)\n",
        "\n",
        "* Visualize the eigenvalues in order from highest to lowest, connecting them with a line. Upon visual inspection, keep all the components whose eigenvalue falls above the point where the slope of the line changes the most drastically, also called the “elbow”\n",
        "\n",
        "* plot the cumulative explained variance as a function of the number of components/dimensions and check for an elbow in the curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBQ9SxxAIoy6"
      },
      "source": [
        "# YOUR CODE HERE: Visualize Explained variance as a function of the number of dimensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_PzLRTnqY9A"
      },
      "source": [
        "#### Plotting the variance percentage at each principal component\n",
        "\n",
        "Cumulative sum is used to display the total sum of data as it grows with each component (or any other series or progression). It is view of the total contribution so far of a given measure against principal components.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaC9QE0rrAWZ"
      },
      "source": [
        "# Finding the variance between the each principal component\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "tot = sum(variance)\n",
        "var_exp = [(i / tot)*100 for i in sorted(variance, reverse=True)]\n",
        "plt.bar(range(1,len(var_exp)+1), var_exp)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Percentage of variance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFV32o44r444"
      },
      "source": [
        "# Print the 'var_exp' and select the no of principal components where the highest variance is preserved\n",
        "print(var_exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJOw9zbpoyVM"
      },
      "source": [
        "From the above plot, the first 7 principal components together contain 91% of the information. So, reducing the dimensionality to about 7 number of principal components  wouldn’t lose too much explained variance. So, remaining components can safely be dropped without losing much information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoN_FiG2p_Wf"
      },
      "source": [
        "#### PCA for Compression\n",
        "\n",
        "After dimensionality reduction, the training set takes up much less space. This size reduction can speed up a classification algorithm tremendously.\n",
        "\n",
        "we will find low-dimensional representations that retain as much information as possible and minimize the compression loss where, we will be looking at minimizing the squared reconstruction error $||X - \\bar{X}||^ 2$ between the original data and its projection.\n",
        "\n",
        "We can also decompress the reduced dataset back to original dimensions by applying the inverse transformation of the PCA projection.\n",
        "\n",
        "In the following code:\n",
        "\n",
        "1. First we iterate through the different number of principal components\n",
        "2. Perform PCA on a range of selected number of components by using `PCA(n_components)`. Here, `n_components` represents how many optimum features need to be used to represent the data without having data loss.\n",
        "3. By applying `pca.fit_transform` method compress the dimensionality of the original dataset\n",
        "4. Then by using `pca.inverse_transform` method decompresses it back to original 30 dimensions\n",
        "5. Compute the reconstruction error (MSE), between the original dataset and the decompressed data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_E1VZOLZ7o"
      },
      "source": [
        "def mse(predict, actual):\n",
        "    \"\"\"Helper function for computing the mean squared error (MSE)\"\"\"\n",
        "    return np.square(predict - actual).sum(axis=1).mean()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScAH5R6pNCfT"
      },
      "source": [
        "loss = []\n",
        "\n",
        "# Iterate over different numbers of principal components, and compute the MSE\n",
        "for num_component in range(1, 31):\n",
        "    pca = PCA(n_components=num_component)\n",
        "\n",
        "    # YOUR CODE HERE: To fit PCA on the scaled data\n",
        "\n",
        "    # Reconstruct the original data\n",
        "    reconst_data = pca.inverse_transform(reduced_data)\n",
        "\n",
        "    # YOUR CODE HERE: Compute the reconstruction error between the original and the projected data\n",
        "    \n",
        "loss = np.asarray(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C43LuDnVxdk0"
      },
      "source": [
        "Here the data compression is not lossless. The data after decompression won't be exactly the same as the original. It will be an approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rycJO24uTN_"
      },
      "source": [
        "How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
        "\n",
        "Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Support Vector classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIZF5SePqTiP"
      },
      "source": [
        "### Plotting MSE vs no of principal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jy9cQJWRDHh"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 8))\n",
        "plt.plot(loss[:,0], loss[:,1]);\n",
        "plt.xticks(np.arange(1, 31, 1))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Mean Square Error')\n",
        "plt.title('MSE vs number of principal components')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed5Lnm0Vq1IY"
      },
      "source": [
        "From the above plot, we can observe as the number of principal components increases, which is the number of eigenvectors used to build the feature space, we can see the mean square error is decreasing. We also observe that when the number of components approaches the number of features that we use to build the feature space, the error is close to 0. The greater number of of principal components we use, the smaller will our reconstruction error be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XSZMdlMu28Q"
      },
      "source": [
        "#### Apply PCA after selecting the '7' principal components\n",
        "\n",
        "Here `n_components` represents how many optimum features need to be used to represent the 30 dimensional data without having data loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG3srfr-ozAY"
      },
      "source": [
        "# Perform PCA after selecting the optimum no. of principal components\n",
        "pca = PCA(n_components=7)\n",
        "\n",
        "# YOUR CODE HERE: Fit the original data\n",
        "\n",
        "# YOUR CODE HERE: Reconstruct back the original data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Yfxtk3Uqgh"
      },
      "source": [
        "print(reconst_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCvgiDUQTdn_"
      },
      "source": [
        "You can compare the reduced data using PCA from scikit learn with the mathematical approach for projecting the data to low-dimensional subspace and reconstructing the data back to original data of 30 features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ui_C-f7mCl"
      },
      "source": [
        "#### Visualization of first 2 principal components in 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAsDu58d6qMt"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_eDa3P-zlrU"
      },
      "source": [
        "### Split the data into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHXDysGjzAR5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(pca_transformed, encoded_labels, random_state=1)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhWcJD991eAI"
      },
      "source": [
        "### Train the Support Vector Classifier with reduced data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuqstUis1i3I"
      },
      "source": [
        "clf = SVC(kernel='linear')\n",
        "\n",
        "# YOUR CODE HERE: Train the model\n",
        "\n",
        "# YOUR CODE HERE: Get the prediction on the test set\n",
        "\n",
        "# YOUR CODE HERE: Calculate the accuracy\n",
        "\n",
        "print(\"Accuracy of test data is\",accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoFCyicDLeP1"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zJ5iUWULiIW"
      },
      "source": [
        "#### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm7IVK0EkPGi"
      },
      "source": [
        "# YOUR CODE HERE: To print the classification report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgz9bfQRLno-"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE_b5DXolOL-"
      },
      "source": [
        "mat = confusion_matrix(Y_test, predicted)\n",
        "# Visualizing the confusion matrix as a heatmap\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d')\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCUz0mCt7AT8"
      },
      "source": [
        "## Support Vector Machine - Regression (SVR)\n",
        "\n",
        "To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting\n",
        "margin violations, SVM Regression tries to fit as many instances as possible\n",
        "on the street while limiting margin violations (i.e., instances off the street). The width of the street is controlled by a hyperparameter, ε\n",
        "\n",
        "![Image](https://www.saedsayad.com/images/SVR_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0KWj_EVTdOd"
      },
      "source": [
        "## Problem Statement: Boston house price prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W9qzzAJTwbp"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price. \n",
        "\n",
        "Following are the details of each feature/attribute of the given dataset.\n",
        "\n",
        "In the dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column MEDV (price)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkpj5kYrUU1D"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The Boston House Price Dataset involves the prediction of a house price in thousands of dollars given details of the house and its neighborhood.\n",
        "\n",
        "The dataset contains 506 rows and 14 columns. It consists of price of houses in various locations in Boston. Along with price, the dataset also provides information such as :\n",
        "\n",
        "* CRIM - per capita crime rate by town.\n",
        "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "* INDUS - proportion of non-retail business acres per town.\n",
        "* CHAS- Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
        "* NOX - nitrogen oxides concentration (parts per 10 million).\n",
        "* RM - average number of rooms per dwelling.\n",
        "* AGE - proportion of owner-occupied units built prior to 1940.\n",
        "* DIS - weighted mean of distances to five Boston employment centres.\n",
        "* RAD - index of accessibility to radial highways.\n",
        "* TAX - full-value property-tax rate per $10,000$.\n",
        "* PTRATIO - pupil-teacher ratio by town.\n",
        "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
        "* LSTAT - Percentage of lower status of the population\n",
        "* MEDV (price) - Median value of owner-occupied homes in $1000s - target column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK9fwApIU3Rm"
      },
      "source": [
        "#### Loading the Boston dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy-LRoWHTrL1"
      },
      "source": [
        "boston_data = pd.read_csv(\"Housing_data.csv\")\n",
        "boston_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0X0PoD1bne6"
      },
      "source": [
        "# Check the shape of dataframe\n",
        "boston_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iNiVHBzUDD"
      },
      "source": [
        "#### Check for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ups59-zWNA"
      },
      "source": [
        "# YOUR CODE HERE: Check for missing values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWmxNzpS0Prr"
      },
      "source": [
        "There is no missing values in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1_pfE1-VZjU"
      },
      "source": [
        "# Viewing the data statistics\n",
        "boston_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeKRFx9tVgY5"
      },
      "source": [
        "# Finding out the correlation between the features\n",
        "corr = boston_data.corr()\n",
        "corr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m89QcxvL0Qgn"
      },
      "source": [
        "#### Visualization of Correlation Matrix\n",
        "\n",
        "From the below matrix, we observe that the highly correlated features has the correlation value close to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2P5eh5lWK4Y"
      },
      "source": [
        "# Plotting the heatmap of correlation between features\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(corr, cmap = 'Wistia', annot= True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JXxy7EaXZJe"
      },
      "source": [
        "# YOUR CODE HERE: Spliting target variable and independent variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNSpmTETbp-f"
      },
      "source": [
        "# Reshape the target variable as the MinMaxScaler expects 2D array\n",
        "y = y.reshape(len(y), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDux0Jdtxshq"
      },
      "source": [
        "### Normalizing the data\n",
        "\n",
        "For the above dataset you can observe that the ranges of different continuous variables are different. This is actually problematic. Therefore, we perform normalization for continuous variable.\n",
        "  \n",
        "The goal of normalization is to change the values of numeric columns in the dataset to a common scale without distorting differences in the ranges of values. We normalize the data to bring all the variables to the same range.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY14fJu4bip8"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "X = sc.fit_transform(X)\n",
        "y = sc.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IhEQdrt6gOq"
      },
      "source": [
        "### Split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5B653kX0Nd"
      },
      "source": [
        "# YOUR CODE HERE: Splitting to training and testing data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2H74o_HYZ9X"
      },
      "source": [
        "### Fit the SVM Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7YfeSuDlSy_"
      },
      "source": [
        "#### Train the model using 'linear' kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7R2GgXelMsV"
      },
      "source": [
        "# Import SVM Regressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# YOUR CODE HERE: Create a SVM Regressor using 'linear' kernel\n",
        "\n",
        "# YOUR CODE HERE: Train the linear model using the training sets \n",
        "\n",
        "# YOUR CODE HERE: Model prediction on train data using 'linear' kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8L-HvQ1oLaM"
      },
      "source": [
        "#### Visualization of Actual vs Predicted prices (Linear Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA5-1KzLlu_T"
      },
      "source": [
        "# YOUR CODE HERE: Visualizing the differences between actual prices and predicted values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNWdl1oaZERL"
      },
      "source": [
        "#### Model Evaluation \n",
        "\n",
        "$R^2$  is a measure of the linear relationship between X and y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.  $R^2$  is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it's mean. 100% indicated that model explains all the variablity of the response data around the mean.\n",
        "\n",
        "$R^2$ = 1−$\\frac{SSE}{SST}$\n",
        " \n",
        "SSE = Sum of Square Error\n",
        "\n",
        "SST = Sum of Square Total\n",
        "\n",
        "$SSE = \\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        "\n",
        "$SST = \\sum_{i=1}^{m} (y_{i}−\\bar{y}_{i})^2$\n",
        "\n",
        " \n",
        "Here  $\\hat{y}$  is predicted value and  $\\bar{y}$  is mean value of  y\n",
        "\n",
        "**RMSE:** Root mean squared error (RMSE) is the square root of the mean of the square of all of the error. RMSE is a way to measure the accuracy, but only to compare prediction errors of different models or model configurations for a particular variable and not between variables, as it is scale-dependent. We compute Root Mean Square Error using formula.\n",
        "\n",
        "$RMSE = \\frac{1}{n} \\sum_{i=1}^{n} ({S_i} − O_{i})^2$\n",
        "\n",
        "\n",
        "where $O_i$ are the observations, $S_i$ predicted values of a variable, and 'n'\n",
        "the number of observations available for analysis.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LAOfS9bl6fK"
      },
      "source": [
        "# YOUR CODE HERE: Predicting Test data with the linear model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN1aoitZmBiK"
      },
      "source": [
        "# Model Evaluation\n",
        "R_square = r2_score(y_test, y_test_pred_linear)\n",
        "print('R square on SVR Linear Kernel:', R_square)\n",
        "\n",
        "print('Root Mean Square Error on SVR Linear Kernel:',np.sqrt(mean_squared_error(y_test, y_test_pred_linear)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHfk4apNprIG"
      },
      "source": [
        "The R square value denotes the accuracy of the model. As the accuracy of the model increases, the R square value reaches close to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rydY-NiiYmZ2"
      },
      "source": [
        "#### Train the model using 'rbf' kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAuaZODJYgJA"
      },
      "source": [
        "# YOUR CODE HERE: Create a SVM Regressor using 'rbf' kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTLeLrVhYtBW"
      },
      "source": [
        "# YOUR CODE HERE:Train the model using the training sets "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmdrmVrnYxnT"
      },
      "source": [
        "The follwing are the details of the parameters used in the SVM Regressor:\n",
        "\n",
        "C : float, optional (default=1.0): The penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
        "\n",
        "kernel : string, optional (default='rbf’): kernel parameters selects the type of hyperplane used to separate the data. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed’ or a callable.\n",
        "\n",
        "degree : int, optional (default=3): Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
        "\n",
        "gamma : float, optional (default='auto’): It is for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set. Current default is 'auto' which uses 1 / n_features.\n",
        "\n",
        "coef0 : float, optional (default=0.0): Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n",
        "\n",
        "shrinking : boolean, optional (default=True): Whether to use the shrinking heuristic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvLGrklaZCGS"
      },
      "source": [
        "# YOUR CODE HERE: Model prediction on train data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ZG-WekZitI"
      },
      "source": [
        "#### Visualization of Actual vs Predicted prices (RBF Kernel)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFIOwK2GZhZR"
      },
      "source": [
        "# Visualizing the differences between actual prices and predicted values\n",
        "plt.scatter(y_train, y_pred_rbf)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted prices\")\n",
        "plt.title(\"Actual Prices vs Predicted prices\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y766x_PZvGA"
      },
      "source": [
        "# Predicting Test data with the model\n",
        "y_test_pred_rbf = rbf_regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7utvFt1Z0ed"
      },
      "source": [
        "#### Model Evaluation for the test data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enhpkgElZ4Ng"
      },
      "source": [
        "# YOUR CODE HERE: Model Evaluation - compute  R square and root mean square error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFfVBAyWPCL"
      },
      "source": [
        "#### Consider the following statement about Principal Component Analysis (PCA) and answer Q.1.\n",
        "\n",
        "A. PCA is a supervised dimensionality reduction technique\n",
        "\n",
        "B. All Principal components are orthogonal to each other\n",
        "\n",
        "C. PCA computes the direction in which the data has the largest variance\n",
        "\n",
        "D. PCA is non-linear dimensionality reduction technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0fE8R6Ud-s8"
      },
      "source": [
        "#@title Q.1.Which of the above options is/are true regarding Principal Component Analysis (PCA)? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer1 = \"\" #@param [\"\",\"Only A\", \"Only B\", \"Both B and C\", \"Both C and D\", \"All of the above\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsKm6rEDrwQi"
      },
      "source": [
        "#### Consider the following problem and answer Q.2.\n",
        "\n",
        "##### For the given 2D data (X, Y) = (1, 6), (2, 7), (3, 8), (4, 9), (5, 10), or represented as follows: \n",
        "\\\\\n",
        "\\begin{bmatrix}\n",
        "1 & 6   \\\\\n",
        "2 & 7   \\\\\n",
        "3 & 8   \\\\\n",
        "4 & 9   \\\\\n",
        "5 & 10   \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "\\\\\n",
        "Compute the following :-\n",
        "\n",
        "1. The covariance matrix\n",
        "\n",
        "2. The eigenvalues and eigenvectors of the covariance matrix\n",
        "\n",
        "The first Principal Component (eigenvector with highest variance) are given as follows:\n",
        "\n",
        "A. \\begin{bmatrix}\n",
        "-\\frac{1}{\\sqrt2}  \\\\\n",
        "-\\frac{1}{\\sqrt2} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "B. \\begin{bmatrix}\n",
        "-\\frac{1}{2}  \\\\\n",
        "-\\frac{1}{2} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "C. \\begin{bmatrix}\n",
        "\\phantom{-}\\frac{1}{2}  \\\\\n",
        "-\\frac{1}{2} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "D. \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt2}  \\\\\n",
        "\\frac{1}{\\sqrt2} \\\\\n",
        "\\end{bmatrix}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlSp1X_GeF41"
      },
      "source": [
        "#@title Q.2. Which is the first Principal Component (eigenvector along which the variance of the data is the highest) for the given dataset from the choices above computed from the Covariance Matrix? Please manually solve the problem yourself and compare it with experimental results.{ run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer2 = \"\" #@param [\"\",\"A\", \"B\", \"C\", \"D\", \"None of the above\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}